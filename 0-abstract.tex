\begin{abstract}


%  Big Topic Intro  
Machine learning models- deep neural networks in particular- have performed remarkably well on small-scale benchmark datasets across a wide variety of domains. However, the ease of finding adversarial counterexamples remains a persistent problem when training times are measured in hours or days and the time to find an adversarial counter-example is measured in seconds.
%  Problem Statement and Drawbacks  
Much work has gone into generating and defending against these false counter-examples, however the cost of a given decision is rarely discussed, while current verification procedures are computationally infeasible at the scales required by safety-critical standards.
%  Our solution and Advantages of our solution  
By using accelerated failure rate models, worst-case examples, and a cost-aware analysis, we can quickly, precisely, and accurately reject a particular change during routine model training procedures rather than relying on real-world use.
%  Conclusion  
Through a survey of many pre-processing techniques, attacks, and ResNet configurations, we show that deeper models do offer marginal gains in survival times compared to their more shallow counterparts, but that this gain is driven more by the model query time than inherent robustness due to model depth.


%  Old abstract is commented out below
% Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available.

% The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models---goals that are often at odds with one another.

%  Big Topic Intro
%  Problem Statement
%  Drawbacks of the other solutions and
%  Our solution
%  Advantages of our solution
%  Conclusion

% In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. 

% Using a novel , we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. 

% We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. 

% To encapsulate the model's robustness and computational cost, we provide a method that uses induced failures to model the probability of failure as a function of time and relate that to a novel metric that allows us to quickly determine whether or not the cost of training a model outweighs the cost of attacking it. 

% Using this approach, we are able to approximate the expected failure rate using a small number of specially crafted samples rather than increasingly larger benchmark datasets. 

% We demonstrate the efficacy of this technique on several datasets using 8-, 16-, 32-, and 64-bit floating-point numbers, various data pre-processing techniques, and several attacks on five configurations of the ResNet model. 

% Then, using empirical measurements, we examine the various trade-offs between cost, robustness, latency, and reliability to find that larger models do not significantly aid in adversarial robustness despite costing significantly more to train.
\end{abstract}