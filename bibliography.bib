@article{adversarialpatch,
	title        = {Adversarial patch},
	author       = {
		Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n
		and Gilmer, Justin
	},
	year         = 2017,
	journal      = {arXiv:1712.09665}
}

@article{aft_models,
	title        = {Survival analysis part II: multivariate data analysis},
	author       = {
		Bradburn, Mike J and Clark, Taane G and Love, Sharon B and Altman, Douglas
		Graham
	},
	year         = 2003,
	journal      = {British journal of cancer},
	publisher    = {Nature Publishing Group},
	volume       = 89,
	number       = 3,
	pages        = {431--436}
}

@inproceedings{ahn2003captcha,
	title        = {CAPTCHA: Using hard AI problems for security},
	author       = {Ahn, Luis von and Blum, Manuel and Hopper, Nicholas J and Langford, John},
	year         = 2003,
	booktitle    = {
		International conference on the theory and applications of cryptographic
		techniques
	},
	pages        = {294--311},
	organization = {Springer}
}

@inproceedings{ai_aviation,
	title        = {
		A comparative study of machine learning techniques for aviation applications
	},
	author       = {Maheshwari, Apoorv and Davendralingam, Navindran and DeLaurentis, Daniel A},
	year         = 2018,
	booktitle    = {2018 Aviation Technology, Integration, and Operations Conference},
	pages        = 3980
}

@article{ai_industry,
	title        = {Machine learning in industrial control system (ICS) security},
	author       = {Koay, Abigail MY and Ko, Ryan K L and Hettema, Hinne and Radke, Kenneth},
	year         = 2023,
	journal      = {Journal of Intelligent Information Systems},
	publisher    = {Springer},
	volume       = 60,
	number       = 2,
	pages        = {377--405}
}

@article{ai_luggage,
	title        = {Modern computer vision techniques for x-ray testing in baggage inspection},
	author       = {
		Mery, Domingo and Svec, Erick and Arias, Marco and Riffo, Vladimir and
		Saavedra, Jose M and Banerjee, Sandipan
	},
	year         = 2016,
	journal      = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	publisher    = {IEEE},
	volume       = 47,
	number       = 4,
	pages        = {682--692}
}

@article{ai_medical_imaging,
	title        = {Machine learning for medical imaging},
	author       = {
		Erickson, Bradley J and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline,
		Timothy L
	},
	year         = 2017,
	journal      = {Radiographics},
	publisher    = {Radiological Society of North America},
	volume       = 37,
	number       = 2,
	pages        = {505--515}
}

@article{ai_prison,
	title        = {Machine learning and criminal justice},
	author       = {
		Travaini, Guido Vittorio and Pacchioni, Federico and Bellumore, Silvia and
		Bosia, Marta and De Micco, Francesco
	},
	year         = 2022,
	journal      = {International journal of environmental research and public health},
	publisher    = {MDPI},
	volume       = 19,
	number       = 17,
	pages        = 10594
}

@inproceedings{ai_security,
	title        = {Dos and don'ts of machine learning in computer security},
	author       = {
		Arp, Daniel and Quiring, Erwin and Pendlebury, Feargus and Warnecke,
		Alexander and Pierazzi, Fabio and Wressnegger, Christian and Cavallaro,
		Lorenzo and Rieck, Konrad
	},
	year         = 2022,
	booktitle    = {31st USENIX Security Symposium (USENIX Security 22)},
	pages        = {3971--3988}
}

@inproceedings{al2017deep,
	title        = {Deep learning algorithm for autonomous driving using {GoogLeNet}},
	author       = {
		Al-Qizwini, Mohammed and Barjasteh, Iman and Al-Qassab, Hothaifa and Radha,
		Hayder
	},
	year         = 2017,
	booktitle    = {2017 IEEE Intelligent Vehicles Symposium (IV)},
	pages        = {89--96},
	organization = {IEEE}
}

@article{art2018,
	title        = {Adversarial Robustness Toolbox v1.2.0},
	author       = {
		Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh~Ngoc and Buesser, Beat
		and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and
		Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian and
		Edwards, Ben
	},
	year         = 2018,
	journal      = {CoRR},
	volume       = {1807.01069}
}

@article{athalye_obfuscated_2018,
	title        = {
		Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}:
		{Circumventing} {Defenses} to {Adversarial} {Examples}
	},
	shorttitle   = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	author       = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	year         = 2018,
	month        = jul,
	journal      = {arXiv:1802.00420 [cs]},
	url          = {http://arxiv.org/abs/1802.00420},
	urldate      = {2020-10-01},
	abstract     = {
		We identify obfuscated gradients, a kind of gradient masking, as a phenomenon
		that leads to a false sense of security in defenses against adversarial
		examples. While defenses that cause obfuscated gradients appear to defeat
		iterative optimizationbased attacks, we ﬁnd defenses relying on this effect
		can be circumvented. We describe characteristic behaviors of defenses
		exhibiting the effect, and for each of the three types of obfuscated
		gradients we discover, we develop attack techniques to overcome it. In a case
		study, examining noncertiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd
		obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on
		obfuscated gradients. Our new attacks successfully circumvent 6 completely,
		and 1 partially, in the original threat model each paper considers.
	},
	language     = {en},
	keywords     = {
		Computer Science - Artificial Intelligence, Computer Science - Cryptography
		and Security, Computer Science - Machine Learning
	}
}

@book{aviation,
	title        = {
		Developing safety-critical software: a practical guide for aviation software
		and DO-178C compliance
	},
	author       = {Rierson, Leanna},
	year         = 2017,
	publisher    = {CRC Press}
}

@inproceedings{aviation_software,
	title        = {Applying lessons from safety-critical systems to security-critical software},
	author       = {Axelrod, C Warren},
	year         = 2011,
	booktitle    = {2011 IEEE Long Island Systems, Applications and Technology Conference},
	pages        = {1--6},
	organization = {IEEE}
}

@article{bailly2022effects,
	title        = {
		Effects of dataset size and interactions on the prediction performance of
		logistic regression and deep learning models
	},
	author       = {
		Bailly, Alexandre and Blanc, Corentin and Francis, {\'E}lie and Guillotin,
		Thierry and Jamal, Fadi and Wakim, B{\'e}chara and Roy, Pascal
	},
	year         = 2022,
	journal      = {Computer Methods and Programs in Biomedicine},
	publisher    = {Elsevier},
	volume       = 213,
	pages        = 106504
}

@article{banks2018driver,
	title        = {
		Driver error or designer error: Using the Perceptual Cycle Model to explore
		the circumstances surrounding the fatal Tesla crash on 7th May 2016
	},
	author       = {Banks, Victoria A and Plant, Katherine L and Stanton, Neville A},
	year         = 2018,
	journal      = {Safety science},
	publisher    = {Elsevier},
	volume       = 108,
	pages        = {278--285}
}

@article{bect_bayesian_2017,
	title        = {Bayesian subset simulation},
	author       = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
	year         = 2017,
	month        = jan,
	journal      = {SIAM/ASA Journal on Uncertainty Quantification},
	volume       = 5,
	number       = 1,
	pages        = {762--786},
	doi          = {10.1137/16M1078276},
	issn         = {2166-2525},
	url          = {http://arxiv.org/abs/1601.02557},
	urldate      = {2020-07-20},
	abstract     = {
		We consider the problem of estimating a probability of failure α, deﬁned as
		the volume of the excursion set of a function f : X ⊆ Rd → R above a given
		threshold, under a given probability measure on X. In this article, we
		combine the popular subset simulation algorithm (Au and Beck, Probab. Eng.
		Mech. 2001) and our sequential Bayesian approach for the estimation of a
		probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat.
		Comput. 2012). This makes it possible to estimate α when the number of
		evaluations of f is very limited and α is very small. The resulting algorithm
		is called Bayesian subset simulation (BSS). A key idea, as in the subset
		simulation algorithm, is to estimate the probabilities of a sequence of
		excursion sets of f above intermediate thresholds, using a sequential Monte
		Carlo (SMC) approach. A Gaussian process prior on f is used to deﬁne the
		sequence of densities targeted by the SMC algorithm, and drive the selection
		of evaluation points of f to estimate the intermediate probabilities.
		Adaptive procedures are proposed to determine the intermediate thresholds and
		the number of evaluations to be carried out at each stage of the algorithm.
		Numerical experiments illustrate that BSS achieves signiﬁcant savings in the
		number of function evaluations with respect to other Monte Carlo approaches.
	},
	language     = {en},
	keywords     = {Statistics - Computation}
}

@inproceedings{bernal2017safety++,
	title        = {
		Safety++ designing {IoT} and wearable systems for industrial safety through a
		user centered design approach
	},
	author       = {
		Bernal, Guillermo and Colombo, Sara and Al Ai Baky, Mohammed and Casalegno,
		Federico
	},
	year         = 2017,
	booktitle    = {
		Proceedings of the 10th International Conference on Pervasive Technologies
		Related to Assistive Environments
	},
	pages        = {163--170}
}

@article{biggio_evasion_2013,
	title        = {Evasion attacks against machine learning at test time},
	author       = {
		Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and
		{\v{S}}rndi{\'c}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli,
		Fabio
	},
	year         = 2013,
	booktitle    = {
		Machine Learning and Knowledge Discovery in Databases: European Conference,
		ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings,
		Part III 13
	},
	pages        = {387--402},
	organization = {Springer}
}

@inproceedings{biggio_multiple_2009,
	title        = {Multiple {Classifier} {Systems} for {Adversarial} {Classification} {Tasks}},
	author       = {Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	year         = 2009,
	booktitle    = {Multiple {Classifier} {Systems}},
	publisher    = {Springer},
	address      = {Berlin, Heidelberg},
	series       = {Lecture {Notes} in {Computer} {Science}},
	pages        = {132--141},
	doi          = {10.1007/978-3-642-02326-2_14},
	isbn         = {978-3-642-02326-2},
	abstract     = {
		Pattern classification systems are currently used in security applications
		like intrusion detection in computer networks, spam filtering and biometric
		identity recognition. These are adversarial classification problems, since
		the classifier faces an intelligent adversary who adaptively modifies
		patterns (e.g., spam e-mails) to evade it. In these tasks the goal of a
		classifier is to attain both a high classification accuracy and a high
		hardness of evasion, but this issue has not been deeply investigated yet in
		the literature. We address it under the viewpoint of the choice of the
		architecture of a multiple classifier system. We propose a measure of the
		hardness of evasion of a classifier architecture, and give an analytical
		evaluation and comparison of an individual classifier and a classifier
		ensemble architecture. We finally report an experimental evaluation on a spam
		filtering task.
	},
	language     = {en},
	editor       = {Benediktsson, Jón Atli and Kittler, Josef and Roli, Fabio}
}

@article{biggio_poisoning_2013,
	title        = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	author       = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	year         = 2013,
	month        = mar,
	journal      = {arXiv:1206.6389 [cs, stat]},
	urldate      = {2020-11-02},
	abstract     = {
		We investigate a family of poisoning attacks against Support Vector Machines
		(SVM). Such attacks inject specially crafted training data that increases the
		SVM's test error. Central to the motivation for these attacks is the fact
		that most learning algorithms assume that their training data comes from a
		natural or well-behaved distribution. However, this assumption does not
		generally hold in security-sensitive settings. As we demonstrate, an
		intelligent adversary can, to some extent, predict the change of the SVM's
		decision function due to malicious input and use this ability to construct
		malicious data. The proposed attack uses a gradient ascent strategy in which
		the gradient is computed based on properties of the SVM's optimal solution.
		This method can be kernelized and enables the attack to be constructed in the
		input space even for non-linear kernels. We experimentally demonstrate that
		our gradient ascent procedure reliably identifies good local maxima of the
		non-convex validation error surface, which significantly increases the
		classifier's test error.
	},
	keywords     = {
		Computer Science - Cryptography and Security, Computer Science - Machine
		Learning, Statistics - Machine Learning
	}
}

@inproceedings{bloom2017self,
	title        = {
		Self-driving cars and data collection: Privacy perceptions of networked
		autonomous vehicles
	},
	author       = {Bloom, Cara and Tan, Joshua and Ramjohn, Javed and Bauer, Lujo},
	year         = 2017,
	booktitle    = {Symposium on Usable Privacy and Security (SOUPS)}
}

@article{blumer1989learnability,
	title        = {Learnability and the Vapnik-Chervonenkis dimension},
	author       = {
		Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth,
		Manfred K
	},
	year         = 1989,
	journal      = {Journal of the ACM},
	volume       = 36,
	number       = 4,
	pages        = {929--965}
}

@inproceedings{braking,
	title        = {Autonomous braking system via deep reinforcement learning},
	author       = {
		Hyunmin Chae and Chang Mook Kang and ByeoungDo Kim and Jaekyum Kim and Chung
		Choo Chung and Jun Won Choi
	},
	year         = 2017,
	booktitle    = {
		{IEEE} 20th International conference on intelligent transportation systems
		({ITSC})
	}
}

@inproceedings{buolamwini2018gender,
	title        = {
		Gender shades: Intersectional accuracy disparities in commercial gender
		classification
	},
	author       = {Buolamwini, Joy and Gebru, Timnit},
	year         = 2018,
	booktitle    = {Conference on fairness, accountability and transparency},
	pages        = {77--91},
	organization = {PMLR}
}

@article{carlini_towards_2017,
	title        = {Towards evaluating the robustness of neural networks},
	author       = {Carlini, Nicholas and Wagner, David},
	year         = 2017,
	booktitle    = {2017 ieee symposium on security and privacy (sp)},
	pages        = {39--57},
	organization = {Ieee}
}

@article{chakraborty_adversarial_2018,
	title        = {Adversarial Attacks and Defences: {A} Survey},
	author       = {
		Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay,
		Anupam and Mukhopadhyay, Debdeep
	},
	year         = 2018,
	journal      = {arXiv:1810.00069 [cs, stat]}
}

@article{chambolle2004algorithm,
	title        = {An algorithm for total variation minimization and applications},
	author       = {Chambolle, Antonin},
	year         = 2004,
	journal      = {Journal of Mathematical imaging and vision},
	publisher    = {Springer},
	volume       = 20,
	number       = 1,
	pages        = {89--97}
}

@article{ching2017opportunities,
	title        = {Opportunities and obstacles for deep learning in biology and medicine},
	author       = {
		Ching, Travers and Himmelstein, Daniel~S. and Beaulieu-Jones, Brett~K. and
		Kalinin, Alexandr~A. and Do, Brian~T. and Way, Gregory~P. and Ferrero, Enrico
		and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael~M. and Xie,
		Wei and Rosen, Gail~L. and Lengerich, Benjamin~J. and Israeli, Johnny and
		Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne~E. and
		Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan~M. and Lavender,
		Christopher~A. and Turaga, Srinivas~C. and Alexandari, Amr~M. and Lu, Zhiyong
		and Harris, David~J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul
		and Peng, Yifan and Wiley, Laura~K. and Segler, Marwin~H.~S. and Boca,
		Simina~M. and Swamidass, S.~ Joshua and Huang, Austin and Gitter, Anthony and
		Greene, Casey~S.
	},
	year         = 2017,
	journal      = {Journal of the Royal Society Interface},
	volume       = 15,
	number       = 141
}

@inproceedings{choquette2021label,
	title        = {Label-only membership inference attacks},
	author       = {
		Choquette-Choo, Christopher A and Tramer, Florian and Carlini, Nicholas and
		Papernot, Nicolas
	},
	year         = 2021,
	booktitle    = {International conference on machine learning},
	pages        = {1964--1974},
	organization = {PMLR}
}

@inproceedings{chou2023applicability,
	title        = {
		Applicability of Deep Learning Model Trainings on Embedded GPU Devices: An
		Empirical Study
	},
	author       = {Chou, Po-Hsuan and Wang, Chao and Mei, Chih-Shuo},
	year         = 2023,
	booktitle    = {2023 12th Mediterranean Conference on Embedded Computing (MECO)},
	pages        = {1--4},
	organization = {IEEE}
}

@article{christmann_robustness_nodate,
	title        = {
		On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for
		{Pattern} {Recognition}
	},
	author       = {Christmann, Andreas and Steinwart, Ingo},
	pages        = 28,
	abstract     = {
		The paper brings together methods from two disciplines: machine learning
		theory and robust statistics. We argue that robustness is an important aspect
		and we show that many existing machine learning methods based on the convex
		risk minimization principle have − besides other good properties − also the
		advantage of being robust. Robustness properties of machine learning methods
		based on convex risk minimization are investigated for the problem of pattern
		recognition. Assumptions are given for the existence of the inﬂuence function
		of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic
		regression, support vector machines, least squares and the AdaBoost loss
		function are treated as special cases. Some results on the robustness of such
		methods are also obtained for the sensitivity curve and the maxbias, which
		are two other robustness criteria. A sensitivity analysis of the support
		vector machine is given.
	},
	language     = {en}
}

@article{chung1988labelings,
	title        = {Labelings of graphs},
	author       = {Chung, Fan RK},
	year         = 1988,
	journal      = {Selected topics in graph theory},
	volume       = 3,
	pages        = {151--168}
}

@article{cifar,
	title        = {Learning multiple layers of features from tiny images},
	author       = {Krizhevsky, Alex and Hinton, Geoffrey and others},
	year         = 2009,
	publisher    = {Toronto, ON, Canada}
}

@inproceedings{cintas_detecting_2020,
	title        = {
		Detecting {Adversarial} {Attacks} via {Subset} {Scanning} of {Autoencoder}
		{Activations} and {Reconstruction} {Error}
	},
	author       = {
		Cintas, Celia and Speakman, Skyler and Akinwande, Victor and Ogallo, William
		and Weldemariam, Komminist and Sridharan, Srihari and McFowland, Edward
	},
	year         = 2020,
	month        = jul,
	booktitle    = {
		Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on
		{Artificial} {Intelligence}
	},
	address      = {Yokohama, Japan},
	pages        = {876--882},
	doi          = {10.24963/ijcai.2020/122},
	isbn         = {978-0-9992411-6-5},
	url          = {https://www.ijcai.org/proceedings/2020/122},
	urldate      = {2020-11-03},
	abstract     = {
		Reliably detecting attacks in a given set of inputs is of high practical
		relevance because of the vulnerability of neural networks to adversarial
		examples. These altered inputs create a security risk in applications with
		real-world consequences, such as self-driving cars, robotics and ﬁnancial
		services. We propose an unsupervised method for detecting adversarial attacks
		in inner layers of autoencoder (AE) networks by maximizing a non-parametric
		measure of anomalous node activations. Previous work in this space has shown
		AE networks can detect anomalous images by thresholding the reconstruction
		error produced by the ﬁnal layer. Furthermore, other detection methods rely
		on data augmentation or specialized training techniques which must be
		asserted before training time. In contrast, we use subset scanning methods
		from the anomalous pattern detection domain to enhance detection power
		without labeled examples of the noise, retraining or data augmentation
		methods. In addition to an anomalous “score” our proposed method also returns
		the subset of nodes within the AE network that contributed to that score.
		This will allow future work to pivot from detection to visualisation and
		explainability. Our scanning approach shows consistently higher detection
		power than existing detection methods across several adversarial noise models
		and a wide range of perturbation strengths.
	},
	language     = {en}
}

@article{colbrook2021can,
	title        = {Can stable and accurate neural networks be computed},
	author       = {Colbrook, Matthew~J. and Antun, Vegard and Hansen, Anders~C.},
	year         = 2021,
	journal      = {On the barriers of deep learning and Smale’s 18th problem. arXiv},
	volume       = 2101
}

@incollection{collett2023modelling,
	title        = {Modelling survival data},
	author       = {Collett, David},
	year         = 2015,
	booktitle    = {Modelling survival data in medical research},
	publisher    = {Springer}
}

@article{concordance,
	title        = {
		A practical perspective on the concordance index for the evaluation and
		selection of prognostic time-to-event models
	},
	author       = {Longato, Enrico and Vettoretti, Martina and Di Camillo, Barbara},
	year         = 2020,
	journal      = {Journal of biomedical informatics},
	publisher    = {Elsevier},
	volume       = 108,
	pages        = 103496
}

@article{corsaro1982something,
	title        = {
		Something old and something new: The importance of prior ethnography in the
		collection and analysis of audiovisual data
	},
	author       = {Corsaro, William A},
	year         = 1982,
	journal      = {Sociological Methods \& Research},
	publisher    = {Sage Publications},
	volume       = 11,
	number       = 2,
	pages        = {145--166}
}

% number={1},
@article{cosentino2019search,
	title        = {The search for sparse, robust neural networks},
	author       = {Cosentino, Justin and Zaiter, Federico and Pei, Dan and Zhu, Jun},
	year         = 2019,
	journal      = {arXiv:1912.02386}
}

@article{croce_reliable_2020,
	title        = {
		Reliable evaluation of adversarial robustness with an ensemble of diverse
		parameter-free attacks
	},
	author       = {Croce, Francesco and Hein, Matthias},
	year         = 2020,
	booktitle    = {International conference on machine learning},
	pages        = {2206--2216},
	organization = {PMLR}
}

@article{croce2020robustbench,
	title        = {RobustBench: a standardized adversarial robustness benchmark},
	author       = {
		Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and
		Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal,
		Prateek and Matthias Hein
	},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.09670}
}

@article{daya_graph-based_2019,
	title        = {A {Graph}-{Based} {Machine} {Learning} {Approach} for {Bot} {Detection}},
	author       = {
		Daya, Abbas Abou and Salahuddin, Mohammad A. and Limam, Noura and Boutaba,
		Raouf
	},
	year         = 2019,
	month        = feb,
	journal      = {arXiv:1902.08538 [cs]},
	url          = {http://arxiv.org/abs/1902.08538},
	urldate      = {2020-11-03},
	language     = {en}
}

@inproceedings{deepfool,
	title        = {Deepfool: a simple and accurate method to fool deep neural networks},
	author       = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	year         = 2016,
	booktitle    = {
		Proceedings of the IEEE conference on computer vision and pattern recognition
	},
	pages        = {2574--2582}
}

@article{desislavov2021compute,
	title        = {Compute and energy consumption trends in deep learning inference},
	author       = {
		Desislavov, Radosvet and Mart{\'\i}nez-Plumed, Fernando and
		Hern{\'a}ndez-Orallo, Jos{\'e}
	},
	year         = 2021,
	journal      = {arXiv:2109.05472}
}

@inproceedings{discretization,
	title        = {
		Defending against whitebox adversarial attacks via randomized discretization
	},
	author       = {Zhang, Yuchen and Liang, Percy},
	year         = 2019,
	booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
	pages        = {684--693},
	organization = {PMLR}
}

@article{distributed_attacks,
	title        = {
		Machine Learning Approaches for Combating Distributed Denial of Service
		Attacks in Modern Networking Environments
	},
	author       = {Aljuhani, Ahamed},
	year         = 2021,
	journal      = {IEEE Access},
	volume       = 9,
	pages        = {42236--42264},
	doi          = {10.1109/ACCESS.2021.3062909}
}

@inproceedings{dohmatob_generalized_2019,
	title        = {Generalized {No} {Free} {Lunch} {Theorem} for {Adversarial} {Robustness}},
	author       = {Dohmatob, Elvis},
	year         = 2019,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	series       = {PMLR},
	volume       = 97
}

@misc{dvc,
	title        = {{DVC}--{Data Version Control}},
	author       = {{DVC Authors}},
	year         = 2023,
	url          = {https://github.com/iterative/dvc.org},
	howpublished = {Github}
}

@article{evans2001gender,
	title        = {
		Gender and age influence on fatality risk from the same physical impact
		determined using two-car crashes
	},
	author       = {Evans, Leonard and Gerrish, Peter H},
	year         = 2001,
	journal      = {SAE transactions},
	publisher    = {JSTOR},
	pages        = {1336--1341}
}

@article{feature_squeezing,
	title        = {Feature squeezing: Detecting adversarial examples in deep neural networks},
	author       = {Xu, Weilin and Evans, David and Qi, Yanjun},
	year         = 2017,
	journal      = {arXiv:1704.01155}
}

@article{fgm,
	title        = {Explaining and harnessing adversarial examples},
	author       = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
	year         = 2014,
	journal      = {arXiv:1412.6572}
}

@article{finlayson2018adversarial,
	title        = {Adversarial Attacks Against Medical Deep Learning Systems},
	author       = {
		Finlayson, Samuel G and Chung, Hyung Won and Kohane, Isaac S and Beam, Andrew
		L
	},
	year         = 2018,
	journal      = {arXiv:1804.05296}
}

@article{formal_adversarial,
	title        = {
		Adversarial robustness of deep neural networks: A survey from a formal
		verification perspective
	},
	author       = {
		Meng, Mark Huasong and Bai, Guangdong and Teo, Sin Gee and Hou, Zhe and Xiao,
		Yan and Lin, Yun and Dong, Jin Song
	},
	year         = 2022,
	journal      = {IEEE Transactions on Dependable and Secure Computing},
	publisher    = {IEEE}
}

@inproceedings{fredrikson_model_2015,
	title        = {
		Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and
		{Basic} {Countermeasures}
	},
	author       = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	year         = 2015,
	booktitle    = {
		Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and
		{Communications} {Security} - {CCS} '15
	},
	publisher    = {ACM Press},
	address      = {Denver, Colorado, USA},
	pages        = {1322--1333},
	doi          = {10.1145/2810103.2813677},
	isbn         = {978-1-4503-3832-5},
	url          = {http://dl.acm.org/citation.cfm?doid=2810103.2813677},
	urldate      = {2020-11-02},
	abstract     = {
		Machine-learning (ML) algorithms are increasingly utilized in
		privacy-sensitive applications such as predicting lifestyle choices, making
		medical diagnoses, and facial recognition. In a model inversion attack,
		recently introduced in a case study of linear classiﬁers in personalized
		medicine by Fredrikson et al. [13], adversarial access to an ML model is
		abused to learn sensitive genomic information about individuals. Whether
		model inversion attacks apply to settings outside theirs, however, is
		unknown.
	},
	language     = {en}
}

@article{fukuda1992theory,
	title        = {Theory and applications of neural networks for industrial control systems},
	author       = {Fukuda, Toshio and Shibata, Takanori},
	year         = 1992,
	journal      = {IEEE Transactions on industrial electronics},
	publisher    = {IEEE},
	volume       = 39,
	number       = 6,
	pages        = {472--489}
}

@inproceedings{gauss_aug,
	title        = {Efficient Defenses Against Adversarial Attacks},
	author       = {Zantedeschi, Valentina and Nicolae, Maria-Irina and Rawat, Ambrish},
	year         = 2017,
	booktitle    = {
		Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
	},
	location     = {Dallas, Texas, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {AISec '17},
	pages        = {39–49},
	isbn         = 9781450352024,
	numpages     = 11,
	keywords     = {deep neural network, model security, defenses, adversarial learning}
}

@inproceedings{gauss_out,
	title        = {
		On the effectiveness of small input noise for defending against query-based
		black-box attacks
	},
	author       = {Byun, Junyoung and Go, Hyojun and Kim, Changick},
	year         = 2022,
	booktitle    = {
		Proceedings of the IEEE/CVF winter conference on applications of computer
		vision
	},
	pages        = {3051--3060}
}

@article{gichoya2022ai,
	title        = {AI recognition of patient race in medical imaging: a modelling study},
	author       = {
		Gichoya, Judy Wawira and Banerjee, Imon and Bhimireddy, Ananth Reddy and
		Burns, John L and Celi, Leo Anthony and Chen, Li-Ching and Correa, Ramon and
		Dullerud, Natalie and Ghassemi, Marzyeh and Huang, Shih-Cheng and others
	},
	year         = 2022,
	journal      = {The Lancet Digital Health},
	publisher    = {Elsevier},
	volume       = 4,
	number       = 6,
	pages        = {e406--e414}
}

@article{grigorescu2020survey,
	title        = {A survey of deep learning techniques for autonomous driving},
	author       = {
		Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel
	},
	year         = 2020,
	journal      = {Journal of Field Robotics},
	publisher    = {Wiley Online Library},
	volume       = 37,
	number       = 3,
	pages        = {362--386}
}

@article{hadj2018continuation,
	title        = {
		Continuation of {Nesterov}’s smoothing for regression with structured
		sparsity in high-dimensional neuroimaging
	},
	author       = {
		Hadj-Selem, Fouad and L{\"o}fstedt, Tommy and Dohmatob, Elvis and Frouin,
		Vincent and Dubois, Mathieu and Guillemot, Vincent and Duchesnay, Edouard
	},
	year         = 2018,
	journal      = {IEEE Transactions on Medical Imaging},
	publisher    = {IEEE},
	volume       = 37,
	number       = 11,
	pages        = {2403--2413}
}

@inproceedings{hamming,
	title        = {
		Global robustness evaluation of deep neural networks with provable guarantees
		for the hamming distance
	},
	author       = {
		Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening,
		Daniel and Kwiatkowska, Marta
	},
	year         = 2019,
	organization = {IJCAI}
}

% volume={},
% number={},
@article{high_conf,
	title        = {
		Lie to Me: A Soft Threshold Defense Method for Adversarial Examples of Remote
		Sensing Images
	},
	author       = {Chen, Li and Xiao, Jun and Zou, Pu and Li, Haifeng},
	year         = 2021,
	journal      = {IEEE Geoscience and Remote Sensing Letters},
	pages        = {1--5},
	doi          = {10.1109/LGRS.2021.3096244}
}

@article{hochreiter1998vanishing,
	title        = {
		The vanishing gradient problem during learning recurrent neural nets and
		problem solutions
	},
	author       = {Hochreiter, Sepp},
	year         = 1998,
	journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	publisher    = {World Scientific},
	volume       = 6,
	number       = {02},
	pages        = {107--116}
}

@inbook{hoffstein_pipher_silverman_2010,
	title        = {Complexity Theory and P vs NP},
	author       = {Hoffstein, Jeffrey and Pipher, Jill and Silverman, Joseph H.},
	year         = 2010,
	booktitle    = {An introduction to mathematical cryptography},
	publisher    = {Springer},
	pages        = {258--262},
	chapter      = 4
}

@inproceedings{hopskipjump,
	title        = {{HopSkipJumpAttack}: A query-efficient decision-based attack},
	author       = {Chen, Jianbo and Jordan, Michael I and Wainwright, Martin J},
	year         = 2020,
	booktitle    = {{IEEE} symposium on security and privacy (sp)},
	pages        = {1277--1294},
	organization = {IEEE}
}

@misc{hydra,
	title        = {Hydra -- A framework for elegantly configuring complex applications},
	author       = {Omry Yadan},
	year         = 2019,
	url          = {https://github.com/facebookresearch/hydra},
	howpublished = {Github}
}

@article{ici,
	title        = {
		Graphical calibration curves and the integrated calibration index (ICI) for
		survival models
	},
	author       = {Austin, Peter C and Harrell Jr, Frank E and van Klaveren, David},
	year         = 2020,
	journal      = {Statistics in Medicine},
	publisher    = {Wiley Online Library},
	volume       = 39,
	number       = 21,
	pages        = {2714--2742}
}

@article{icoh,
	title        = {GLOBAL ESTIMATES OF OCCUPATIONAL  ACCIDENTS AND WORK-RELATED ILLNESSES 2017},
	author       = {ICOH},
	year         = 2017,
	journal      = {International Commission on Occupational Health`},
	publisher    = {ICOH},
	url          = {
		http://www.icohweb.org/site/images/news/pdf/Report\%20Global\%20Estimates\%20of\%20Occupational\%20Accidents\%20and\%20Work-related\%20Illnesses\%202017\%20rev1.pdf
	}
}

@book{IEC61508,
	title        = {IEC 61508 Safety and Functional Safety},
	author       = {\mbox{International Electrotechnical Commission}},
	year         = 2010,
	publisher    = {International Electrotechnical Commission},
	edition      = {2nd}
}

@book{IEC62034,
	title        = {IEC 62304 Medical Device Software - Software Life Cycle Processes},
	author       = {\mbox{International Electrotechnical Commission}},
	year         = 2006,
	publisher    = {International Electrotechnical Commission},
	edition      = {2nd}
}

@article{imageenhance,
	title        = {Relationship between entropy and SNR changes in image enhancement},
	author       = {Krbcova, Zuzana and Kukal, Jaromir},
	year         = 2017,
	journal      = {EURASIP Journal on Image and Video Processing},
	publisher    = {Springer},
	volume       = 2017,
	number       = 1,
	pages        = {1--8}
}

@article{inkawhich_adversarial_2018,
	title        = {
		Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition}
		{Classifiers}
	},
	author       = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	year         = 2018,
	month        = nov,
	journal      = {arXiv:1811.11875 [cs]},
	url          = {http://arxiv.org/abs/1811.11875},
	urldate      = {2020-09-17},
	abstract     = {
		The success of deep learning research has catapulted deep models into
		production systems that our society is becoming increasingly dependent on,
		especially in the image and video domains. However, recent work has shown
		that these largely uninterpretable models exhibit glaring security
		vulnerabilities in the presence of an adversary. In this work, we develop a
		powerful untargeted adversarial attack for action recognition systems in both
		white-box and black-box settings. Action recognition models differ from
		image-classiﬁcation models in that their inputs contain a temporal dimension,
		which we explicitly target in the attack. Drawing inspiration from image
		classiﬁer attacks, we create new attacks which achieve state-of-the-art
		success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We
		ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with
		sparsely and imperceptibly perturbed examples. We also demonstrate the
		transferability of our attacks to black-box action recognition systems.
	},
	language     = {en},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{intermittent,
	title        = {
		Timing is Almost Everything: Realistic Evaluation of the Very Short
		Intermittent DDoS Attacks
	},
	author       = {Park, Jeman and Nyang, DaeHun and Mohaisen, Aziz},
	year         = 2018,
	booktitle    = {2018 16th Annual Conference on Privacy, Security and Trust (PST)},
	pages        = {1--10},
	doi          = {10.1109/PST.2018.8514210}
}

@misc{iso26262,
	title        = {{ISO} 26262-1:2011, Road vehicles --- Functional safety},
	author       = {\mbox{International~Standards~Organization}},
	year         = 2018,
	howpublished = {
		\href{https://www.iso.org/standard/43464.html}{https://www.iso.org/standard/43464.html}
	}
}

@book{jahan2016multi,
	title        = {
		Multi-criteria decision analysis for supporting the selection of engineering
		materials in product design
	},
	author       = {Jahan, Ali and Edwards, Kevin L and Bahraminasab, Marjan},
	year         = 2016,
	publisher    = {Butterworth-Heinemann}
}

@inproceedings{jakubovitz2018improving,
	title        = {
		Improving dnn robustness to adversarial attacks using jacobian regularization
	},
	author       = {Jakubovitz, Daniel and Giryes, Raja},
	year         = 2018,
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV)},
	pages        = {514--529}
}

@inproceedings{jang_objective_2017,
	title        = {
		Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial}
		{Examples} in {Machine} {Learning}
	},
	author       = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	year         = 2017,
	month        = dec,
	booktitle    = {
		Proceedings of the 33rd {Annual} {Computer} {Security} {Applications}
		{Conference}
	},
	publisher    = {ACM},
	address      = {Orlando FL USA},
	pages        = {262--277},
	doi          = {10.1145/3134600.3134635},
	isbn         = {978-1-4503-5345-8},
	url          = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	urldate      = {2020-11-02},
	abstract     = {
		Fueled by massive amounts of data, models produced by machinelearning (ML)
		algorithms are being used in diverse domains where security is a concern,
		such as, automotive systems, finance, health-care, computer vision, speech
		recognition, naturallanguage processing, and malware detection. Of particular
		concern is use of ML in cyberphysical systems, such as driverless cars and
		aviation, where the presence of an adversary can cause serious consequences.
		In this paper we focus on attacks caused by adversarial samples, which are
		inputs crafted by adding small, often imperceptible, perturbations to force a
		ML model to misclassify. We present a simple gradient-descent based algorithm
		for finding adversarial samples, which performs well in comparison to
		existing algorithms. The second issue that this paper tackles is that of
		metrics. We present a novel metric based on few computer-vision algorithms
		for measuring the quality of adversarial samples.
	},
	language     = {en}
}

@article{jian2022pruning,
	title        = {Pruning Adversarially Robust Neural Networks without Adversarial Examples},
	author       = {
		Jian, Tong and Wang, Zifeng and Wang, Yanzhi and Dy, Jennifer and Ioannidis,
		Stratis
	},
	year         = 2022,
	journal      = {arXiv:2210.04311}
}

@misc{k8s,
	title        = {Kubernetes--an open source system for managing containerized applications},
	author       = {Kubernetes},
	year         = 2019,
	month        = jun,
	url          = {https://github.com/kubernetes/kubernetes},
	howpublished = {Github}
}

@misc{k8s-size,
	title        = {Octoverse Projects},
	author       = {Github},
	year         = 2019,
	journal      = {The State of the Octoverse},
	publisher    = {Github.com},
	url          = {https://octoverse.github.com/2018/projects.html}
}

@article{kamal2017study,
	title        = {
		A study on the security of password hashing based on gpu based, password
		cracking using high-performance cloud computing
	},
	author       = {Kamal, Parves},
	year         = 2017
}

@book{kleinbaum1996survival,
	title        = {Survival analysis a self-learning text},
	author       = {Kleinbaum, David G and Klein, Mitchel},
	year         = 1996,
	publisher    = {Springer}
}

@article{ko_loss-driven_2019,
	title        = {
		Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line}
		{Time} {Series} {Analysis}
	},
	author       = {
		Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park,
		Saerom
	},
	year         = 2019,
	month        = jan,
	journal      = {Sustainability},
	volume       = 11,
	number       = 12,
	pages        = 3489,
	doi          = {10.3390/su11123489},
	url          = {https://www.mdpi.com/2071-1050/11/12/3489},
	urldate      = {2020-11-02},
	copyright    = {http://creativecommons.org/licenses/by/3.0/},
	note         = {Number: 12 Publisher: Multidisciplinary Digital Publishing Institute},
	abstract     = {
		Developing a robust and sustainable system is an important problem in which
		deep learning models are used in real-world applications. Ensemble methods
		combine diverse models to improve performance and achieve robustness. The
		analysis of time series data requires dealing with continuously incoming
		instances; however, most ensemble models suffer when adapting to a change in
		data distribution. Therefore, we propose an on-line ensemble deep learning
		algorithm that aggregates deep learning models and adjusts the ensemble
		weight based on loss value in this study. We theoretically demonstrate that
		the ensemble weight converges to the limiting distribution, and, thus,
		minimizes the average total loss from a new regret measure based on
		adversarial assumption. We also present an overall framework that can be
		applied to analyze time series. In the experiments, we focused on the on-line
		phase, in which the ensemble models predict the binary class for the
		simulated data and the financial and non-financial real data. The proposed
		method outperformed other ensemble approaches. Moreover, our method was not
		only robust to the intentional attacks but also sustainable in data
		distribution changes. In the future, our algorithm can be extended to
		regression and multiclass classification problems.
	},
	language     = {en},
	keywords     = {
		adaptive learning, ensemble deep learning, on-line learning, time series
		analysis
	}
}

@article{koch2021reduced,
	title        = {
		Reduced, reused and recycled: The life of a dataset in machine learning
		research
	},
	author       = {Koch, Bernard and Denton, Emily and Hanna, Alex and Foster, Jacob G},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2112.01716}
}

@inproceedings{label_smoothing,
	title        = {Adversarial Perturbations of Deep Neural Networks},
	author       = {D. Warde-Farley and I. Goodfellow},
	year         = 2017,
	booktitle    = {Perturbations, Optimization, and Statistics},
	publisher    = {The MIT Press, Cambridge, Massachusetts},
	editor       = {T. Hazan , G. Papandreou, D. Tarlow}
}

@article{lachin1981introduction,
	title        = {
		Introduction to sample size determination and power analysis for clinical
		trials
	},
	author       = {Lachin, John M},
	year         = 1981,
	journal      = {Controlled clinical trials},
	publisher    = {Elsevier},
	volume       = 2,
	number       = 2,
	pages        = {93--113}
}

@inproceedings{lam2004new,
	title        = {New design-to-test software strategies accelerate time-to-market},
	author       = {Lam, Hau},
	year         = 2004,
	booktitle    = {
		IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology
		Symposium (IEEE Cat. No. 04CH37585)
	},
	pages        = {140--143},
	organization = {IEEE}
}

@article{lambert2004parametric,
	title        = {
		Parametric accelerated failure time models with random effects and an
		application to kidney transplant survival
	},
	author       = {Lambert, Philippe and Collett, Dave and Kimber, Alan and Johnson, Rachel},
	year         = 2004,
	journal      = {Statistics in medicine},
	publisher    = {Wiley Online Library},
	volume       = 23,
	number       = 20,
	pages        = {3177--3192}
}

@article{lawless1995methods,
	title        = {
		Methods for the estimation of failure distributions and rates from automobile
		warranty data
	},
	author       = {Lawless, Jerry and Hu, Joan and Cao, Jin},
	year         = 1995,
	journal      = {Lifetime Data Analysis},
	publisher    = {Springer},
	volume       = 1,
	pages        = {227--240}
}

@inproceedings{leurent2020sha,
	title        = {
		{SHA-1} is a shambles: First {Chosen-Prefix} collision on {SHA-1} and
		application to the {PGP} web of trust
	},
	author       = {Leurent, Ga{\"e}tan and Peyrin, Thomas},
	year         = 2020,
	booktitle    = {29th USENIX security symposium (USENIX security 20)},
	pages        = {1839--1856}
}

@article{li_general_2016,
	title        = {
		A {General} {Retraining} {Framework} for {Scalable} {Adversarial}
		{Classification}
	},
	author       = {Li, Bo and Vorobeychik, Yevgeniy and Chen, Xinyun},
	year         = 2016,
	month        = nov,
	journal      = {arXiv:1604.02606 [cs, stat]},
	url          = {http://arxiv.org/abs/1604.02606},
	urldate      = {2020-09-17},
	abstract     = {
		Traditional classiﬁcation algorithms assume that training and test data come
		from similar distributions. This assumption is violated in adversarial
		settings, where malicious actors modify instances to evade detection. A
		number of custom methods have been developed for both adversarial evasion
		attacks and robust learning. We propose the ﬁrst systematic and
		general-purpose retraining framework which can: a) boost robustness of an
		arbitrary learning algorithm, in the face of b) a broader class of
		adversarial models than any prior methods. We show that, under natural
		conditions, the retraining framework minimizes an upper bound on optimal
		adversarial risk, and show how to extend this result to account for
		approximations of evasion attacks. Extensive experimental evaluation
		demonstrates that our retraining methods are nearly indistinguishable from
		state-of-the-art algorithms for optimizing adversarial risk, but are more
		general and far more scalable. The experiments also conﬁrm that without
		retraining, our adversarial framework dramatically reduces the effectiveness
		of learning. In contrast, retraining signiﬁcantly boosts robustness to
		evasion attacks without signiﬁcantly compromising overall accuracy.
	},
	language     = {en},
	keywords     = {
		Computer Science - Computer Science and Game Theory, Computer Science -
		Machine Learning, Statistics - Machine Learning
	}
}

@inproceedings{li2021membership,
	title        = {Membership leakage in label-only exposures},
	author       = {Li, Zheng and Zhang, Yang},
	year         = 2021,
	booktitle    = {
		Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications
		Security
	},
	pages        = {880--895}
}

@article{lifelines,
	title        = {lifelines: survival analysis in Python},
	author       = {Cameron Davidson-Pilon},
	year         = 2019,
	journal      = {Journal of Open Source Software},
	publisher    = {The Open Journal},
	volume       = 4,
	number       = 40,
	pages        = 1317
}

@article{liu2013development,
	title        = {
		Development of Accelerated Failure-free Test Method for Automotive Alternator
		Magnet
	},
	author       = {Liu, Qiang and Ismail, Azianti and Jung, Won},
	year         = 2013,
	journal      = {Journal of the Society of Korea Industrial and Systems Engineering},
	publisher    = {Society of Korea Industrial and System Engineering},
	volume       = 36,
	number       = 4,
	pages        = {92--99}
}

@inproceedings{lowd_adversarial_2005,
	title        = {Adversarial learning},
	author       = {Lowd, Daniel and Meek, Christopher},
	year         = 2005,
	month        = aug,
	booktitle    = {
		Proceedings of the eleventh {ACM} {SIGKDD} international conference on
		{Knowledge} discovery in data mining
	},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{KDD} '05},
	pages        = {641--647},
	doi          = {10.1145/1081870.1081950},
	isbn         = {978-1-59593-135-1},
	url          = {https://doi.org/10.1145/1081870.1081950},
	urldate      = {2020-11-02},
	abstract     = {
		Many classification tasks, such as spam filtering, intrusion detection, and
		terrorism detection, are complicated by an adversary who wishes to avoid
		detection. Previous work on adversarial classification has made the
		unrealistic assumption that the attacker has perfect knowledge of the
		classifier [2]. In this paper, we introduce the adversarial classifier
		reverse engineering (ACRE) learning problem, the task of learning sufficient
		information about a classifier to construct adversarial attacks. We present
		efficient algorithms for reverse engineering linear classifiers with either
		continuous or Boolean features and demonstrate their effectiveness using real
		data from the domain of spam filtering.
	},
	keywords     = {adversarial classification, linear classifiers, spam}
}

@article{lu2020gender,
	title        = {Gender bias in neural natural language processing},
	author       = {
		Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and
		Datta, Anupam
	},
	year         = 2020,
	journal      = {
		Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the
		Occasion of His 65th Birthday
	},
	publisher    = {Springer},
	pages        = {189--202}
}

@article{ma2020imbalanced,
	title        = {
		Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness
	},
	author       = {
		Ma, Xingjun and Jiang, Linxi and Huang, Hanxun and Weng, Zejia and Bailey,
		James and Jiang, Yu-Gang
	},
	year         = 2020,
	journal      = {arXiv:2006.13726}
}

@article{madry2017towards,
	title        = {Towards deep learning models resistant to adversarial attacks},
	author       = {
		Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras,
		Dimitris and Vladu, Adrian
	},
	year         = 2017,
	journal      = {arXiv:1706.06083}
}

@article{makary2016medical,
	title        = {Medical error---the third leading cause of death in the {US}},
	author       = {Makary, Martin A and Daniel, Michael},
	year         = 2016,
	journal      = {{BMJ}},
	publisher    = {British Medical Journal Publishing Group},
	volume       = 353
}

@article{meeker1998accelerated,
	title        = {Accelerated degradation tests: modeling and analysis},
	author       = {Meeker, William Q and Escobar, Luis A and Lu, C Joseph},
	year         = 1998,
	journal      = {Technometrics},
	publisher    = {Taylor \& Francis},
	volume       = 40,
	number       = 2,
	pages        = {89--99}
}

@article{meyers,
	title        = {Safety-critical computer vision},
	author       = {Meyers, Charles and L\"{o}fstedt, Tommy and Elmroth, Erik},
	year         = 2023,
	journal      = {Springer Artificial Intelligence Review}
}

@article{miller_adversarial_2020,
	title        = {
		Adversarial {Learning} {Targeting} {Deep} {Neural} {Network}
		{Classification}: {A} {Comprehensive} {Review} of {Defenses} {Against}
		{Attacks}
	},
	shorttitle   = {
		Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}
	},
	author       = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	year         = 2020,
	month        = mar,
	journal      = {Proceedings of the IEEE},
	volume       = 108,
	number       = 3,
	pages        = {402--433},
	doi          = {10.1109/JPROC.2020.2970615},
	issn         = {0018-9219, 1558-2256},
	url          = {https://ieeexplore.ieee.org/document/9013065/},
	urldate      = {2020-08-12},
	language     = {en}
}

@article{min_curious_2020,
	title        = {
		The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data}
		{Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}
	},
	shorttitle   = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	author       = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	year         = 2020,
	month        = jun,
	journal      = {arXiv:2002.11080 [cs, stat]},
	url          = {http://arxiv.org/abs/2002.11080},
	urldate      = {2020-10-02},
	abstract     = {
		Adversarial training has shown its ability in producing models that are
		robust to perturbations on the input data, but usually at the expense of
		decrease in the standard accuracy. To mitigate this issue, it is commonly
		believed that more training data will eventually help such adversarially
		robust models generalize better on the benign/unperturbed test data. In this
		paper, however, we challenge this conventional belief and show that more
		training data can hurt the generalization of adversarially robust models in
		the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture
		classiﬁcation with a linear loss and identify three regimes based on the
		strength of the adversary. In the weak adversary regime, more data improves
		the generalization of adversarially robust models. In the medium adversary
		regime, with more training data, the generalization loss exhibits a double
		descent curve, which implies the existence of an intermediate stage where
		more training data hurts the generalization. In the strong adversary regime,
		more data almost immediately causes the generalization error to increase.
		Then we move to the analysis of a two-dimensional classiﬁcation problem with
		a 0-1 loss. We prove that more data always hurts the generalization
		performance of adversarially trained models with large perturbations. To
		complement our theoretical results, we conduct empirical studies on Gaussian
		mixture classiﬁcation, support vector machines (SVMs), and linear regression.
	},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{mnist,
	title        = {
		The mnist database of handwritten digit images for machine learning research
	},
	author       = {Deng, Li},
	year         = 2012,
	journal      = {IEEE Signal Processing Magazine},
	publisher    = {IEEE},
	volume       = 29,
	number       = 6,
	pages        = {141--142}
}

@article{monmasson2011fpgas,
	title        = {FPGAs in industrial control applications},
	author       = {
		Monmasson, Eric and Idkhajine, Lahoucine and Cirstea, Marcian N and Bahri,
		Imene and Tisan, Alin and Naouar, Mohamed Wissem
	},
	year         = 2011,
	journal      = {IEEE Transactions on Industrial informatics},
	publisher    = {IEEE},
	volume       = 7,
	number       = 2,
	pages        = {224--243}
}

@book{nelson2010behavior,
	title        = {Behavior of machine learning algorithms in adversarial environments},
	author       = {Nelson, Blaine Alan},
	year         = 2010,
	publisher    = {University of California, Berkeley}
}

@misc{nhtsa,
	title        = {
		Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash
		Causation Survey
	},
	author       = {
		National Highway Transportation Safety Administration's (NHTSA) National
		Center for Statistics and Analysis
	},
	year         = 2015,
	publisher    = {US Department of Transportation},
	howpublished = {
		\href{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115}{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115}
	}
}

@article{nsga2,
	title        = {A fast and elitist multiobjective genetic algorithm: NSGA-II},
	author       = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
	year         = 2002,
	journal      = {IEEE transactions on evolutionary computation},
	publisher    = {IEEE},
	volume       = 6,
	number       = 2,
	pages        = {182--197}
}

@misc{OECD,
	title        = {{OECD} statistics},
	author       = {The Organisation for Economic Co-operation and Development},
	year         = 2020,
	journal      = {{OECD} Statistics},
	publisher    = {International Transport Forum},
	howpublished = {\href{https://stats.oecd.org}{https://stats.oecd.org/}}
}

@inproceedings{optuna,
	title        = {Optuna: A next-generation hyperparameter optimization framework},
	author       = {
		Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and
		Koyama, Masanori
	},
	year         = 2019,
	booktitle    = {
		Proceedings of the 25th ACM SIGKDD international conference on knowledge
		discovery \& data mining
	},
	pages        = {2623--2631}
}

@inproceedings{orekondy2019knockoff,
	title        = {Knockoff nets: Stealing functionality of black-box models},
	author       = {Orekondy, Tribhuvanesh and Schiele, Bernt and Fritz, Mario},
	year         = 2019,
	booktitle    = {
		Proceedings of the IEEE/CVF conference on computer vision and pattern
		recognition
	},
	pages        = {4954--4963}
}

@article{pakdemirli2019artificial,
	title        = {
		Artificial intelligence in radiology: friend or foe? Where are we now and
		where are we heading?
	},
	author       = {Pakdemirli, Emre},
	year         = 2019,
	journal      = {Acta radiologica open},
	publisher    = {SAGE Publications Sage UK: London, England},
	volume       = 8,
	number       = 2
}

@article{pantoja2021concordance,
	title        = {Concordance measures and time-dependent ROC methods},
	author       = {
		Pantoja-Galicia, Norberto and Okereke, Olivia I and Blacker, Deborah and
		Betensky, Rebecca A
	},
	year         = 2021,
	journal      = {Biostatistics \& epidemiology},
	publisher    = {Taylor \& Francis},
	volume       = 5,
	number       = 2,
	pages        = {232--249}
}

@article{papernot_distillation_2016,
	title        = {
		Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep}
		{Neural} {Networks}
	},
	author       = {
		Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami,
		Ananthram
	},
	year         = 2016,
	month        = mar,
	journal      = {arXiv:1511.04508 [cs, stat]},
	url          = {http://arxiv.org/abs/1511.04508},
	urldate      = {2020-11-03},
	abstract     = {
		Deep learning algorithms have been shown to perform extremely well on many
		classical machine learning problems. However, recent studies have shown that
		deep learning, like other machine learning techniques, is vulnerable to
		adversarial samples: inputs crafted to force a deep neural network (DNN) to
		provide adversary-selected outputs. Such attacks can seriously undermine the
		security of the system supported by the DNN, sometimes with devastating
		consequences. For example, autonomous vehicles can be crashed, illicit or
		illegal content can bypass content ﬁlters, or biometric authentication
		systems can be manipulated to allow improper access. In this work, we
		introduce a defensive mechanism called defensive distillation to reduce the
		effectiveness of adversarial samples on DNNs. We analytically investigate the
		generalizability and robustness properties granted by the use of defensive
		distillation when training DNNs. We also empirically study the effectiveness
		of our defense mechanisms on two DNNs placed in adversarial settings. The
		study shows that defensive distillation can reduce effectiveness of sample
		creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains
		can be explained by the fact that distillation leads gradients used in
		adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd
		that distillation increases the average minimum number of features that need
		to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs
		we tested.
	},
	language     = {en},
	keywords     = {
		Computer Science - Cryptography and Security, Computer Science - Machine
		Learning, Statistics - Machine Learning, Computer Science - Neural and
		Evolutionary Computing
	}
}

@misc{paretoset,
	title        = {paretoset},
	author       = {tommyod},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	commit       = {9da133c}
}

@article{paudice_detection_2018,
	title        = {
		Detection of {Adversarial} {Training} {Examples} in {Poisoning} {Attacks}
		through {Anomaly} {Detection}
	},
	author       = {
		Paudice, Andrea and Muñoz-González, Luis and Gyorgy, Andras and Lupu, Emil C.
	},
	year         = 2018,
	month        = feb,
	journal      = {arXiv:1802.03041 [cs, stat]},
	url          = {http://arxiv.org/abs/1802.03041},
	urldate      = {2020-11-03},
	abstract     = {
		Machine learning has become an important component for many systems and
		applications including computer vision, spam ﬁltering, malware and network
		intrusion detection, among others. Despite the capabilities of machine
		learning algorithms to extract valuable information from data and produce
		accurate predictions, it has been shown that these algorithms are vulnerable
		to attacks. Data poisoning is one of the most relevant security threats
		against machine learning systems, where attackers can subvert the learning
		process by injecting malicious samples in the training data. Recent work in
		adversarial machine learning has shown that the so-called optimal attack
		strategies can successfully poison linear classiﬁers, degrading the
		performance of the system dramatically after compromising a small fraction of
		the training dataset. In this paper we propose a defence mechanism to
		mitigate the effect of these optimal poisoning attacks based on outlier
		detection. We show empirically that the adversarial examples generated by
		these attack strategies are quite different from genuine points, as no
		detectability constrains are considered to craft the attack. Hence, they can
		be detected with an appropriate pre-ﬁltering of the training dataset.
	},
	language     = {en},
	keywords     = {
		Computer Science - Cryptography and Security, Computer Science - Machine
		Learning, Statistics - Machine Learning
	}
}

@book{pearson2005mining,
	title        = {Mining imperfect data: Dealing with contamination and incomplete records},
	author       = {Pearson, Ronald K},
	year         = 2005,
	publisher    = {SIAM}
}

@article{pixelattack,
	title        = {Adversarial robustness assessment},
	author       = {Kotyan, Shashank and Vargas, Danilo Vasconcellos},
	year         = 2022,
	journal      = {PloS one},
	publisher    = {Public Library of Science San Francisco, CA USA},
	volume       = 17,
	number       = 4,
	pages        = {e0265723}
}

@book{ramirez2000resource,
	title        = {
		A resource guide on racial profiling data collection systems: Promising
		practices and lessons learned
	},
	author       = {Ramirez, Deborah and McDevitt, Jack and Farrell, Amy},
	year         = 2000,
	publisher    = {US Department of Justice}
}

@inproceedings{resnet,
	title        = {Deep residual learning for image recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {
		Proceedings of the IEEE conference on computer vision and pattern recognition
	},
	pages        = {770--778}
}

@article{reverse_sigmoid,
	title        = {Defending Against Model Stealing Attacks Using Deceptive Perturbations},
	author       = {Taesung Lee and Benjamin Edwards and Ian M. Molloy and Dong Su},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1806.00054},
	url          = {http://arxiv.org/abs/1806.00054},
	eprinttype   = {arXiv},
	eprint       = {1806.00054},
	timestamp    = {Wed, 02 Jun 2021 09:13:29 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1806-00054.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{roh2019survey,
	title        = {
		A survey on data collection for machine learning: a big data-ai integration
		perspective
	},
	author       = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
	year         = 2019,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	publisher    = {IEEE},
	volume       = 33,
	number       = 4,
	pages        = {1328--1347}
}

@article{rolnick2017power,
	title        = {The power of deeper networks for expressing natural functions},
	author       = {Rolnick, David and Tegmark, Max},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1705.05502}
}

@inproceedings{ross2018improving,
	title        = {
		Improving the adversarial robustness and interpretability of deep neural
		networks by regularizing their input gradients
	},
	author       = {Ross, Andrew and Doshi-Velez, Finale},
	year         = 2018,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 32
}

@article{rudin1992nonlinear,
	title        = {Nonlinear total variation based noise removal algorithms},
	author       = {Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
	year         = 1992,
	journal      = {Physica D: nonlinear phenomena},
	publisher    = {Elsevier},
	volume       = 60,
	number       = {1-4},
	pages        = {259--268}
}

@article{safetyframework,
	title        = {A framework for safety automation of safety-critical systems operations},
	author       = {Acharyulu, PV Srinivas and Seetharamaiah, P},
	year         = 2015,
	journal      = {Safety Science},
	publisher    = {Elsevier},
	volume       = 77,
	pages        = {133--142}
}

@inproceedings{saha2020hidden,
	title        = {Hidden trigger backdoor attacks},
	author       = {Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
	year         = 2020,
	booktitle    = {Proceedings of the AAAI conference on artificial intelligence},
	volume       = 34,
	number       = {07},
	pages        = {11957--11965}
}

@article{sahiner2019deep,
	title        = {Deep learning in medical imaging and radiation therapy},
	author       = {
		Sahiner, Berkman and Pezeshk, Aria and Hadjiiski, Lubomir M and Wang,
		Xiaosong and Drukker, Karen and Cha, Kenny H and Summers, Ronald M and Giger,
		Maryellen L
	},
	year         = 2019,
	journal      = {Medical physics},
	publisher    = {Wiley Online Library},
	volume       = 46,
	number       = 1,
	pages        = {e1--e36}
}

@article{santos2021universal,
	title        = {
		Universal adversarial attacks on neural networks for power allocation in a
		massive MIMO system
	},
	author       = {
		Santos, Pablo Mill{\'a}n and Manoj, BR and Sadeghi, Meysam and Larsson, Erik
		G
	},
	year         = 2021,
	journal      = {IEEE Wireless Communications Letters},
	publisher    = {IEEE},
	volume       = 11,
	number       = 1,
	pages        = {67--71}
}

@article{schmoor2000sample,
	title        = {
		Sample size considerations for the evaluation of prognostic factors in
		survival analysis
	},
	author       = {Schmoor, Claudia and Sauerbrei, Willi and Schumacher, Martin},
	year         = 2000,
	journal      = {Statistics in medicine},
	publisher    = {Wiley Online Library},
	volume       = 19,
	number       = 4,
	pages        = {441--452}
}

@article{sehwag2019towards,
	title        = {Towards compact and robust deep neural networks},
	author       = {Sehwag, Vikash and Wang, Shiqi and Mittal, Prateek and Jana, Suman},
	year         = 2019,
	journal      = {arXiv:1906.06110}
}

@book{shalev2014understanding,
	title        = {Understanding machine learning: From theory to algorithms},
	author       = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year         = 2014,
	publisher    = {Cambridge university press, Cambridge, UK.}
}

@inproceedings{shokri2017membership,
	title        = {Membership inference attacks against machine learning models},
	author       = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	year         = 2017,
	booktitle    = {2017 IEEE Symposium on Security and Privacy (SP)},
	pages        = {3--18},
	organization = {IEEE}
}

@article{simon-gabriel_first-order_2019,
	title        = {
		First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input}
		{Dimension}
	},
	author       = {
		Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf,
		Bernhard and Lopez-Paz, David
	},
	year         = 2019,
	month        = jun,
	journal      = {arXiv:1802.01421 [cs, stat]},
	url          = {http://arxiv.org/abs/1802.01421},
	urldate      = {2020-09-17},
	abstract     = {
		Over the past few years, neural networks were proven vulnerable to
		adversarial images: targeted but imperceptible image perturbations lead to
		drastically different predictions. We show that adversarial vulnerability
		increases with the gradients of the training objective when viewed as a
		function of the inputs. Surprisingly, vulnerability does not depend on
		network topology: for many standard network architectures, we prove that at
		initialization, the 1-norm of these gradients grows as the square root of the
		input dimension, leaving the networks increasingly vulnerable with growing
		image size. We empirically show that this dimension dependence persists after
		either usual or robust training, but gets attenuated with higher
		regularization.
	},
	language     = {en},
	keywords     = {
		68T45, Computer Science - Computer Vision and Pattern Recognition, Computer
		Science - Machine Learning, I.2.6, Statistics - Machine Learning
	}
}

@inproceedings{sinn2019evolutionary,
	title        = {Evolutionary search for adversarially robust neural networks},
	author       = {Sinn, Mathieu and Wistuba, M and Buesser, B and Nicolae, MI and Tran, M},
	year         = 2019,
	booktitle    = {Safe Machine Learning workshop at ICLR}
}

@book{stats,
	title        = {An introduction to mathematical statistics and its applications},
	author       = {Larsen, Richard J. and Marx, Morris L.},
	year         = 2018,
	publisher    = {Pearson},
	place        = {Boston}
}

@article{stoica2004model,
	title        = {Model-order selection: a review of information criterion rules},
	author       = {Stoica, Petre and Selen, Yngve},
	year         = 2004,
	journal      = {IEEE Signal Processing Magazine},
	publisher    = {IEEE},
	volume       = 21,
	number       = 4,
	pages        = {36--47}
}

@inproceedings{sun2017revisiting,
	title        = {Revisiting unreasonable effectiveness of data in deep learning era},
	author       = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {843--852}
}

@book{taddy2019business,
	title        = {
		Business data science: Combining machine learning and economics to optimize,
		automate, and accelerate business decisions
	},
	author       = {Taddy, Matt},
	year         = 2019,
	publisher    = {McGraw-Hill Education}
}

@inproceedings{testautomation,
	title        = {
		Test automation for safety-critical systems: Industrial application and
		future developments
	},
	author       = {Peleska, Jan},
	year         = 1996,
	booktitle    = {International Symposium of Formal Methods Europe},
	pages        = {39--59},
	organization = {Springer}
}

@inproceedings{thill_online_2017,
	title        = {
		Online anomaly detection on the webscope {S5} dataset: {A} comparative study
	},
	shorttitle   = {Online anomaly detection on the webscope {S5} dataset},
	author       = {Thill, M. and Konen, W. and Bäck, T.},
	year         = 2017,
	month        = may,
	booktitle    = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	pages        = {1--8},
	doi          = {10.1109/EAIS.2017.7954844},
	note         = {ISSN: 2473-4691},
	abstract     = {
		An unresolved challenge for all kind of temporal data is the reliable anomaly
		detection, especially when adaptability is required in the case of
		non-stationary time series or when the nature of future anomalies is unknown
		or only vaguely defined. Most of the current anomaly detection algorithms
		follow the general idea to classify an anomaly as a significant deviation
		from the prediction. In this paper we present a comparative study where
		several online anomaly detection algorithms are compared on the large Yahoo
		Webscope S5 anomaly benchmark. We show that a relatively Simple Online
		Regression Anomaly Detector (SORAD) is quite successful compared to other
		anomaly detectors. We discuss the importance of several adaptive and online
		elements of the algorithm and their influence on the overall anomaly
		detection accuracy.
	},
	keywords     = {
		Benchmark testing, data handling, Detection algorithms, Internet,
		nonstationary time series, online anomaly detection, Prediction algorithms,
		regression analysis, simple online regression anomaly detector, SORAD,
		temporal data, time series, Time series analysis, Training, Training data,
		Transient analysis, Yahoo Webscope S5 anomaly benchmark
	}
}

@article{tiwari_binary_2019,
	title        = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	author       = {Tiwari, Prayag and Melucci, Massimo},
	year         = 2019,
	month        = jul,
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 33,
	pages        = {10051--10052},
	doi          = {10.1609/aaai.v33i01.330110051},
	issn         = {2374-3468},
	url          = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	urldate      = {2020-11-02},
	copyright    = {
		Copyright (c) 2019 Association for the Advancement of Artificial Intelligence
	},
	note         = {Number: 01},
	abstract     = {
		Machine Learning (ML) helps us to recognize patterns from raw data. ML is
		used in numerous domains i.e. biomedical, agricultural, food technology, etc.
		Despite recent technological advancements, there is still room for
		substantial improvement in prediction. Current ML models are based on
		classical theories of probability and statistics, which can now be replaced
		by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In
		this paper, we propose the Binary Classifier Inspired by Quantum Theory
		(BCIQT) model, which outperforms the state of the art classification in terms
		of recall for every category.
	},
	language     = {en}
}

@book{topologytextbook,
	title        = {Introduction to topology},
	author       = {Mendelson, Bert},
	year         = 2012,
	publisher    = {Dover Publications},
	place        = {New York}
}

@article{tramer,
	title        = {The space of transferable adversarial examples},
	author       = {
		Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan
		and McDaniel, Patrick
	},
	year         = 2017,
	journal      = {arXiv:1704.03453}
}

@article{tramer_stealing_nodate,
	title        = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	author       = {
		Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and
		Ristenpart, Thomas
	},
	year         = 2016,
	booktitle    = {25th {USENIX} Security Symposium ({USENIX} Security 16)},
	pages        = 19,
	abstract     = {
		Machine learning (ML) models may be deemed confidential due to their
		sensitive training data, commercial value, or use in security applications.
		Increasingly often, confidential ML models are being deployed with publicly
		accessible query interfaces. ML-as-a-service (“predictive analytics”) systems
		are an example: Some allow users to train models on potentially sensitive
		data and charge others for access on a pay-per-query basis.
	},
	language     = {en}
}

% 1646-1654, 2019.
@article{tsipras_robustness_2019,
	title        = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	author       = {
		Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner,
		Alexander and Madry, Aleksander
	},
	year         = 2019,
	month        = sep,
	journal      = {arXiv:1805.12152 [cs, stat]},
	url          = {http://arxiv.org/abs/1805.12152},
	urldate      = {2020-10-01},
	abstract     = {
		We show that there may exist an inherent tension between the goal of
		adversarial robustness and that of standard generalization. Speciﬁcally,
		training robust models may not only be more resource-consuming, but also lead
		to a reduction of standard accuracy. We demonstrate that this trade-off
		between the standard accuracy of a model and its robustness to adversarial
		perturbations provably exists in a fairly simple and natural setting. These
		ﬁndings also corroborate a similar phenomenon observed empirically in more
		complex settings. Further, we argue that this phenomenon is a consequence of
		robust classiﬁers learning fundamentally different feature representations
		than standard classiﬁers. These differences, in particular, seem to result in
		unexpected beneﬁts: the representations learned by robust models tend to
		align better with salient data characteristics and human perception.
	},
	language     = {en},
	keywords     = {
		Computer Science - Computer Vision and Pattern Recognition, Computer Science
		- Machine Learning, Computer Science - Neural and Evolutionary Computing,
		Statistics - Machine Learning
	}
}

@inproceedings{tuan2010modeling,
	title        = {
		Modeling and verification of safety critical systems: A case study on
		pacemaker
	},
	author       = {Tuan, Luu Anh and Zheng, Man Chun and Tho, Quan Thanh},
	year         = 2010,
	booktitle    = {
		2010 Fourth International Conference on Secure Software Integration and
		Reliability Improvement
	},
	pages        = {23--32},
	organization = {IEEE}
}

@article{uesato_adversarial_2018,
	title        = {
		Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak}
		{Attacks}
	},
	author       = {
		Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli,
		Pushmeet
	},
	year         = 2018,
	month        = jun,
	journal      = {arXiv:1802.05666 [cs, stat]},
	url          = {http://arxiv.org/abs/1802.05666},
	urldate      = {2020-09-28},
	abstract     = {
		This paper investigates recently proposed approaches for defending against
		adversarial examples and evaluating adversarial robustness. We motivate
		adversarial risk as an objective for achieving models robust to worst-case
		inputs. We then frame commonly used attacks and evaluation metrics as deﬁning
		a tractable surrogate objective to the true adversarial risk. This suggests
		that models may optimize this surrogate rather than the true adversarial
		risk. We formalize this notion as obscurity to an adversary, and develop
		tools and heuristics for identifying obscured models and designing
		transparent models. We demonstrate that this is a signiﬁcant problem in
		practice by repurposing gradient-free optimization techniques into
		adversarial attacks, which we use to decrease the accuracy of several
		recently proposed defenses to near zero. Our hope is that our formulations
		and results will help researchers to develop more powerful defenses.
	},
	language     = {en},
	keywords     = {
		Computer Science - Cryptography and Security, Computer Science - Machine
		Learning, Statistics - Machine Learning
	}
}

@article{vadera_assessing_2020,
	title        = {
		Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and
		{Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network}
		{Classification}
	},
	author       = {
		Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin,
		Benjamin M.
	},
	year         = 2020,
	month        = feb,
	journal      = {arXiv:2002.02842 [cs, stat]},
	url          = {http://arxiv.org/abs/2002.02842},
	urldate      = {2020-07-20},
	abstract     = {
		In this paper, we consider the problem of assessing the adversarial
		robustness of deep neural network models under both Markov chain Monte Carlo
		(MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We
		characterize the robustness of each method to two types of adversarial
		attacks: the fast gradient sign method (FGSM) and projected gradient descent
		(PGD). We show that full MCMC-based inference has excellent robustness,
		signiﬁcantly outperforming standard point estimation-based learning. On the
		other hand, BDK provides marginal improvements. As an additional
		contribution, we present a storage-efﬁcient approach to computing adversarial
		examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.
	},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{vapnik1994measuring,
	title        = {Measuring the VC-dimension of a learning machine},
	author       = {Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
	year         = 1994,
	journal      = {Neural computation},
	publisher    = {MIT Press},
	volume       = 6,
	number       = 5,
	pages        = {851--876}
}

@inproceedings{vehicle_formal,
	title        = {
		Formal scenario-based testing of autonomous vehicles: From simulation to the
		real world
	},
	author       = {
		Fremont, Daniel J and Kim, Edward and Pant, Yash Vardhan and Seshia, Sanjit A
		and Acharya, Atul and Bruso, Xantha and Wells, Paul and Lemke, Steve and Lu,
		Qiang and Mehta, Shalin
	},
	year         = 2020,
	booktitle    = {
		2020 IEEE 23rd International Conference on Intelligent Transportation Systems
		(ITSC)
	},
	pages        = {1--8},
	organization = {IEEE}
}

@inproceedings{vehicle_testing_review,
	title        = {Autonomous vehicles testing methods review},
	author       = {Huang, WuLing and Wang, Kunfeng and Lv, Yisheng and Zhu, FengHua},
	year         = 2016,
	booktitle    = {
		2016 IEEE 19th International Conference on Intelligent Transportation Systems
		(ITSC)
	},
	pages        = {163--168},
	organization = {IEEE}
}

@article{vgg,
	title        = {Very deep convolutional networks for large-scale image recognition},
	author       = {Simonyan, Karen and Zisserman, Andrew},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1409.1556}
}

@article{wang_security_2019,
	title        = {The security of machine learning in an adversarial setting: {A} survey},
	shorttitle   = {The security of machine learning in an adversarial setting},
	author       = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
	year         = 2019,
	month        = aug,
	journal      = {Journal of Parallel and Distributed Computing},
	volume       = 130,
	pages        = {12--23},
	doi          = {10.1016/j.jpdc.2019.03.003},
	issn         = {07437315},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518309183},
	urldate      = {2020-08-12},
	abstract     = {
		Machine learning (ML) methods have demonstrated impressive performance in
		many application fields such as autopilot, facial recognition, and spam
		detection. Traditionally, ML models are trained and deployed in a benign
		setting, in which the testing and training data have identical statistical
		characteristics. However, this assumption usually does not hold in the sense
		that the ML model is designed in an adversarial setting, where some
		statistical properties of the data can be tampered with by a capable
		adversary. Specifically, it has been observed that adversarial examples (also
		known as adversarial input perambulations) elaborately crafted during
		training/test phases can seriously undermine the ML performance. The
		susceptibility of ML models in adversarial settings and the corresponding
		countermeasures have been studied by many researchers in both academic and
		industrial communities. In this work, we present a comprehensive overview of
		the investigation of the security properties of ML algorithms under
		adversarial settings. First, we analyze the ML security model to develop a
		blueprint for this interdisciplinary research area. Then, we review
		adversarial attack methods and discuss the defense strategies against them.
		Finally, relying upon the reviewed work, we provide prospective relevant
		future works for designing more secure ML models.
	},
	language     = {en}
}

@inproceedings{xiao2021improving,
	title        = {
		Improving Transferability of Adversarial Patches on Face Recognition With
		Generative Models
	},
	author       = {
		Xiao, Zihao and Gao, Xianfeng and Fu, Chilin and Dong, Yinpeng and Gao, Wei
		and Zhang, Xiaolu and Zhou, Jun and Zhu, Jun
	},
	year         = 2021,
	booktitle    = {
		Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
		Recognition
	},
	pages        = {11845--11854}
}

@article{yu_bayesian_2016,
	title        = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	author       = {Yu, Edward and Parekh, Parth},
	year         = 2016,
	month        = oct,
	journal      = {arXiv:1610.07677 [cs, stat]},
	url          = {http://arxiv.org/abs/1610.07677},
	urldate      = {2020-07-20},
	abstract     = {
		Methods for unsupervised anomaly detection suﬀer from the fact that the data
		is unlabeled, making it diﬃcult to assess the optimality of detection
		algorithms. Ensemble learning has shown exceptional results in classiﬁcation
		and clustering problems, but has not seen as much research in the context of
		outlier detection. Existing methods focus on combining output scores of
		individual detectors, but this leads to outputs that are not easily
		interpretable. In this paper, we introduce a theoretical foundation for
		combining individual detectors with Bayesian classiﬁer combination. Not only
		are posterior distributions easily interpreted as the probability
		distribution of anomalies, but bias, variance, and individual error rates of
		detectors are all easily obtained. Performance on real-world datasets shows
		high accuracy across varied types of time series data.
	},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{zhang_cross-dataset_2019,
	title        = {Cross-dataset time series anomaly detection for cloud systems},
	author       = {
		Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and
		Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and
		Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and
		Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei
	},
	year         = 2019,
	month        = jul,
	booktitle    = {
		Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual}
		{Technical} {Conference}
	},
	publisher    = {USENIX Association},
	address      = {USA},
	series       = {{USENIX} {ATC} '19},
	pages        = {1063--1076},
	isbn         = {978-1-939133-03-8},
	urldate      = {2020-11-02},
	abstract     = {
		In recent years, software applications are increasingly deployed as online
		services on cloud computing platforms. It is important to detect anomalies in
		cloud systems in order to maintain high service availability. However, given
		the velocity, volume, and diversified nature of cloud monitoring data, it is
		difficult to obtain sufficient labelled data to build an accurate anomaly
		detection model. In this paper, we propose cross-dataset anomaly detection:
		detect anomalies in a new unlabelled dataset (the target) by training an
		anomaly detection model on existing labelled datasets (the source). Our
		approach, called ATAD (Active Transfer Anomaly Detection), integrates both
		transfer learning and active learning techniques. Transfer learning is
		applied to transfer knowledge from the source dataset to the target dataset,
		and active learning is applied to determine informative labels of a small
		part of samples from unlabelled datasets. Through experiments, we show that
		ATAD is effective in cross-dataset time series anomaly detection.
		Furthermore, we only need to label about 1\%-5\% of unlabelled data and can
		still achieve significant performance improvement.
	}
}

@article{ziai_active_2019,
	title        = {Active {Learning} for {Network} {Intrusion} {Detection}},
	author       = {Ziai, Amir},
	year         = 2019,
	month        = apr,
	journal      = {arXiv:1904.01555 [cs, stat]},
	url          = {http://arxiv.org/abs/1904.01555},
	urldate      = {2020-11-02},
	abstract     = {
		Network operators are generally aware of common attack vectors that they
		defend against. For most networks the vast majority of traffic is legitimate.
		However new attack vectors are continually designed and attempted by bad
		actors which bypass detection and go unnoticed due to low volume. One
		strategy for finding such activity is to look for anomalous behavior.
		Investigating anomalous behavior requires significant time and resources.
		Collecting a large number of labeled examples for training supervised models
		is both prohibitively expensive and subject to obsoletion as new attacks
		surface. A purely unsupervised methodology is ideal; however, research has
		shown that even a very small number of labeled examples can significantly
		improve the quality of anomaly detection. A methodology that minimizes the
		number of required labels while maximizing the quality of detection is
		desirable. False positives in this context result in wasted effort or
		blockage of legitimate traffic and false negatives translate to undetected
		attacks. We propose a general active learning framework and experiment with
		different choices of learners and sampling strategies.
	},
	keywords     = {
		Computer Science - Cryptography and Security, Computer Science - Machine
		Learning, Statistics - Machine Learning
	}
}

@article{zirger1996effect,
	title        = {The effect of acceleration techniques on product development time},
	author       = {Zirger, Billie J and Hartley, Janet L},
	year         = 1996,
	journal      = {IEEE Transactions on Engineering Management},
	publisher    = {IEEE},
	volume       = 43,
	number       = 2,
	pages        = {143--152}
}
