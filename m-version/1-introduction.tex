\section{Introduction}
% Feel free to rephrase/reuse any part of it. Please make sure to include some references. 

Machine learning (ML) pipelines are integral components in the development and deployment of accurate and robust models. These pipelines orchestrate the flow of data and computations through a series of stages, including data preprocessing, feature extraction, model training, and inference. The complexity of ML pipelines has grown significantly with the increasing demand for sophisticated models, and their execution often spans diverse CPU and GPU architectures.

One of the foremost challenges in the realm of ML pipelines is the challenge of adversarial attacks. Adversarial attacks are deliberate attempts to compromise the integrity of ML models by subtly altering the input data fed into different stages of the pipeline. These alterations are often imperceptible to human observers but can lead to incorrect predictions, posing serious security risks in applications such as autonomous vehicles, image recognition systems, and natural language processing.

Moreover, another aspect of optimizing ML pipelines involve the relationship between computational cost, model loss, and prediction accuracy. Achieving high prediction accuracy often comes at the expense of increased computational resources, including processing power and memory. Understanding the complicated trade-offs between these factors is vital for deploying ML models in resource- or cost-constrained environments.

To address these challenges, this paper seeks to answer two research questions.

\begin{enumerate}
    \item How can deep neural network robustness against adversarial attacks be improved within ML pipelines?
    \item What is the optimal balance between computational cost, model loss, and prediction accuracy in ML pipelines, particularly in relation to larger models and their performance in both benign and adversarial scenarios?
\end{enumerate}

To answer these questions, we present a series of experiments revealing the robustness of various deep neural networks against adversarial attacks and explore the relationship between computational cost, model loss, and  prediction accuracy in both benign and adversarial contexts. Our experiments reveal that larger models do not, in fact, make better models with regards to either benign or adversarial performance. 
% I would really appreciate it if you can quantify the results here. 

The contribution of this paper is x-fold:
\begin{itemize}
    \item We propose a modelling framework for extending accelerated failure rate analysis to ML models.
    \item We conduct extensive experiments to assess the robustness of various deep neural networks in the face of adversarial attacks.
    \item Our study delves into the relationship between computational cost, model loss, and prediction accuracy. We shed light on whether larger models consistently outperform smaller ones in benign and adversarial contexts, aiding practitioners in making informed decisions about model selection and resource allocation.
\end{itemize}
