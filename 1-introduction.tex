\section{Introduction}

Machine Learning (ML) has become a widely popular tool for solving complex problems across many disciplines, like medical imaging~\citep{ai_medical_imaging}, computer security~\citep{ai_security}, law enforcement~\citep{ai_prison}, aviation~\citep{ai_aviation}, and parcel scanning~\cite{ai_luggage}. Despite this, adversarial attacks aim to exploit vulnerabilities in machine learning models by introducing subtle modifications to data submitted to various stages of the ML pipeline, leading to misclassification or otherwise erroneous outputs~\citep{chakraborty_adversarial_2018}. Ensuring the robustness of ML models against adversaries has become a critical concern~\citep{adversarialpatch,carlini_towards_2017,croce_reliable_2020,hopskipjump,art2018,meyers}. However, understanding the relationship between computational cost and the performance against adversarial noise remains an ongoing challenge.  In this work, we investigated the robustness of various deep neural networks and data pre-processing techniques against adversarial attacks and explored the relationship between computational cost and  prediction accuracy in both benign and adversarial contexts.  By using samples crafted specifically to be challenging and applying accelerated failure rate models (see Sec.~\ref{afr_models}) we provide a method for estimating the expected failure time across the entire feasible adversarial space. Using this model, we demonstrate that larger models, while offering marginal gains over smaller models, do so at the expense of training times that far outpace the expected adversarial training time.

\subsection{Motivations}

It is routine to think in this adversarial context when considering safety- or security- critical applications~\cite{ai_medical_imaging,ai_security,ai_prison,ai_aviation,ai_luggage} where we assume the attacker is operating in their best-case scenario~\cite{leurent2020sha,kamal2017study,madry2017towards,pixelattack,deepfool,croce_reliable_2020}. A recent study~\cite{kamal2017study} distilled the process of password-cracking into an cloud-native service that can break common password schemes in a number of days. Another paper~\cite{leurent2020sha}, defines `broken' in the context of time to highlight the ease with which one can subvert a particular hashing algorithm. However, someone attacking a machine learning model might have a variety of competing goals that optimize the perturbation distance, number of queries, or other metrics~\cite{madry2017towards,hopskipjump,pixelattack,fgm,deepfool}. Much work has gone into mitigating the detrimental effects of these attacks, for example by adding noise in the training process~\cite{gauss_aug,gauss_out}, rejecting low-confidence results without penalty~\cite{high_conf}, or reducing the bit-depth of the data and model weights~\cite{feature_squeezing}. However, these analyses focus on ad-hoc posterior evaluations on benchmark datasets (namely CIFAR-10 and MNIST) to determine whether or not a given technique is more- or less-effective than another. That is, the relationship between marginal benefit and marginal cost is unclear. Furthermore, the community has relied on larger models~\cite{desislavov2021compute} and larger datasets~\cite{desislavov2021compute,bailly2022effects} to yield increasingly marginal gains~\cite{sun2017revisiting}. In order to reach safety-critical standards routine in other industries~\cite{iso26262,IEC61508,IEC62034}, we must move beyond the limited test/train split paradigm that would require many, many billions of samples for every change of a neural network~\cite{meyers}. So, first, we discuss failure rates and cost in the context of deep neural networks (see Section~\ref{cost}). Second, we demonstrate how to model the failure rate as a function of time  using accelerated methods (see Section~\ref{afr_models}). Third, we extend this time-dependent concept of `broken' algorithms from cryptography into machine learning (see Section~\ref{cost_normalization}). Using this, we can quickly and precisely investigate the effects of various attacks (see Section~\ref{attacks}), defences (see Section~\ref{defences}), and model architectures (see Section~\ref{models}).


\subsection{Contributions}

We summarize our contributions below:
\begin{itemize}
	\item We propose an accelerated failure time method for analyzing ML models under adversarial perturbations and provide substantial empirical evidence that this method is both effective and dataset-agnostic, allowing us to predict the expected failure rate more precisely and accurately than with either adversarial or benign accuracy alone.
	\item We use accelerated failure time models to measure model robustness across a wide variety of signal pre-processing techniques to explore the relationships between latency, accuracy, and model depth.
	\item We introduce a metric for evaluating whether or not a model is robust to adversarial attacks in a time- and compute-constrained context.
	\item We use this metric to show that, at best, increasing the number of hidden layers in a neural network increases training time while adding little-to-no benefit in the presence of an adversary.
\end{itemize}
