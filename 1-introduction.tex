\section{Introduction}

Machine Learning (ML) has become a widely popular tool for solving complex problems across many disciplines, like medical imaging (\citep{}). Despite this, adversarial attacks aim to exploit vulnerabilities in machine learning models by introducing subtle modifications to data submitted to various stages of the ML pipeline, leading to misclassification or otherwise erroneous outputs~\citep{chakraborty_adversarial_2018}. Ensuring the robustness of ML models against adversaries has become a critical concern~\citep{adversarialpatch,carlini_towards_2017,croce_reliable_2020,hopskipjump,art2018,meyers}. However, understanding the relationship between computational cost and the performance against adversarial noise remains an ongoing challenge.  In this paper, we  investigate the robustness of various deep neural networks and data pre-processing techniques against adversarial attacks and explore the relationship between computational cost and  prediction accuracy in both benign and adversarial contexts.  By using samples crafted specifically to be challenging and applying accelerated failure rate models (see Sec.~\ref{afr_models}) we provide a way to estimate the expected failure time across the entire feasible adversarial space. Using this model, we demonstrate that larger models, while offering marginal gains over smaller models, do so at the expense of training times that far outpace the expected adversarial training time.

\subsection{Contributions}

We summarize our contributions below:
\begin{itemize}
    \item We propose to use accelerated failure time methods for analyzing ML models under adversarial perturbations and provide substantial empirical evidence that this method is both effective and dataset-agnostic, allowing us to predict the expected failure rate more precisely and accurately than with either adversarial or benign accuracy alone.
    \item We use this method to measure model robustness across a wide variety of signal pre-processing techniques to explore the relationships between latency, up-time, accuracy, and model depth.
    \item We use this method to show that, at best, increasing the number of hidden layers in a neural network increases training time while having little-to-no benefit in the presence of an adversary.
\end{itemize}
