\section{Introduction}

Machine Learning (ML) has become widely popular for solving complex prediction problems across many disciplines, such as medical imaging~\cite{ai_medical_imaging}, computer security~\cite{ai_security}, law enforcement~\cite{ai_prison}, aviation~\cite{ai_aviation}, and logistics~\cite{ai_luggage}. Despite this, adversarial attacks exploit ML models by introducing subtle modifications to data which leads to misclassification or otherwise erroneous outputs~\cite{chakraborty_adversarial_2018}. To ensure the robustness of ML models against adversaries has become a critical concern~\cite{adversarialpatch,carlini_towards_2017,croce_reliable_2020,hopskipjump,art2018,meyers}.

The purpose of this work was to evaluate if survival analysis can predict the success of a particular set of model hyper-parameters. In addition, we explored the relationship between computational cost and prediction accuracy in both benign and adversarial contexts.  By using samples crafted specifically to be challenging and applying survival models (see Section~\ref{aft_models}) we provide a framework to predict the expected failure time across the adversarial space. Using survival models, we demonstrate that larger machine learning models, while offering marginal gains over smaller models, do so at the expense of training times that far outpace the expected survival time and that it is simply not feasible to defend against certain attacks using the examined models and defences.



\subsection{Motivations}

It is routine to consider an adversarial context in safety---or security---critical applications~\cite{ai_medical_imaging,ai_security,ai_prison,ai_aviation,ai_luggage} where we assume the attacker is operating in their own best-case scenario~\cite{leurent2020sha,kamal2017study,madry2017towards,pixelattack,deepfool,croce_reliable_2020}. Cryptography often defines `broken' in the context of time to quantify the feasibility of an attack~\cite{leurent2020sha}---`broken' algorithms are usually defined as those for which attacks can be conducted in a (relatively) small amount of time. For example, one recent study~\cite{kamal2017study} distilled the process of password-cracking into a cloud-based service that can break common password schemes in a number of days. However, someone attacking a machine learning model might have a variety of competing goals (\textit{e.g.}, minimising the perturbation distance or maximising the false confidence)~\cite{madry2017towards,hopskipjump,pixelattack,fgm,deepfool}, so time analyses are less straightforward. What is missing, however, is a method to directly model the effect of attack criteria on the survival time.

Much work has gone into mitigating adversarial attacks, for example by adding noise in the training process~\cite{gauss_aug,gauss_out}, rejecting low-confidence results~\cite{high_conf}, or by reducing the bit-depth of the data and model weights~\cite{feature_squeezing}. However, these analyses focus on ad-hoc posterior evaluations on benchmark datasets (\textit{e.g.} CIFAR-10 or MNIST) to determine whether or not a given technique is more or less effective than another. That is, the relationship between marginal benefit and marginal cost is unclear. Furthermore, the community has trended towards larger models~\cite{desislavov2021compute} and larger datasets~\cite{desislavov2021compute,bailly2022effects} to yield increasingly marginal gains~\cite{sun2017revisiting}. For example, autonomous vehicles still largely rely on system integration tests to verify safety~\cite{vehicle_testing_review}, assuming that human-like accident metrics will guarantee safety. While there are simulation techniques~\cite{vehicle_formal} that highlight problematic scenarios by testing a component in a simulated world in which all components are modelled digitally, implementing them requires building an entire digital world that can nevertheless miss real-world edge cases. Furthermore, while formal methods for neural network verification do exist, they are generally too costly to be feasible for tuning and verifying large scale machine learning models~\cite{formal_adversarial}. 
To reach safety-critical standards that are routine in other industries~\cite{iso26262,IEC61508,IEC62034}, the machine learning field must move beyond the limited test/train split paradigm that would require many, many billions of test samples for every change of a model to meet industry standards~\cite{meyers}.
The proposed method models the complex relationship between model hyper-parameters and the resulting robustness of the model, using nothing more than routine metrics collected in the model tuning stage.

\subsection{Contributions}

The contributions of this work are:
\begin{itemize}
	\item Survival analysis models for analyzing ML models under adversarial perturbations with substantial empirical evidence that survival analysis is both effective and dataset-agnostic, allowing for the expected failure rate to be predicted more precisely and accurately than with either adversarial or benign accuracy alone.
	\item Survival analysis models to measure model robustness across a wide variety of signal pre-processing techniques, exploring the relationships between latency, accuracy, and model depth.
	\item A novel metric: The \textit{training rate and survival heuristic } (TRASH) \textit{for inference and robustness evaluation} (FIRE) to evaluate whether or not a model is robust to adversarial attacks in a time- and compute-constrained context.
	\item Substantial empirical evidence that larger neural networks increase training and prediction time while adding little-to-no benefit in the presence of an adversary.
\end{itemize}
