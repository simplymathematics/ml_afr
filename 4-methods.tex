\section{Methodology}
\label{methods}
Below we outline the experiments performed and the hyper-parameter configurations across the various model architectures, model defences, and attacks. All experiments were conducted on a Ubuntu 18.04 virtual machine running in a shared-host environment with one NVIDIA V100 GPU using Python 3.8.8. All configurations were tested in a grid search using \texttt{hydra} \citep{hydra} to manage the parameters, \texttt{dvc}~\citep{dvc} to ensure reproducibility, and \texttt{optuna}~\citep{optuna} to manage the scheduling. For each attack and model configuration, we collected the metrics outlined in Eq.~\ref{eq:acc}-\ref{eq:cost} as well as inference time, training time, and attack generation time.

\subsection{Dataset}
\label{dataset}
Experiments were performed on both the CIFAR10~\citep{cifar} and MNIST~\citep{mnist} datasets. We measured the adversarial/benign accuracies, the attack generation time, and the prediction time. To calculate the adversarial failure rate and the cost we used Eqs.~\ref{eq:failure_rate}~\&~\ref{eq:cost}. Eq.~\ref{eq:acc}. The data were centered and scaled such that the mean was zero and the standard deviation for each pixel was one. In addition, the data were shuffled to provide us with 10 samples for each configuration. We trained on 80\% of the samples for both datasets. Of the remaining 20\%, 100 were withheld exclusively for the attacks and the rest was used to to test the benign accuracy.

\subsection{Tested Models}
\label{models}
The Residual Neural Network (ResNet)~\citep{resnet} is a popular classification model \footnote{More than 180 thousand citations : \href{https://scholar.google.com/scholar?cites=9281510746729853742}{ResNet citations on Google Scholar.}}. because of its ability to train neural networks with many layers efficiently through \textbf{residual connections}.
\textit{Deep} networks, \textit{e.g.}, VGG~\citep{vgg}, ResNet~\citep{resnet}, rely on the depth of the network, which, despite leading to more accurate or robust results~\citep{rolnick2017power, carlini_towards_2017}, tends to lead to the `vanishing gradient problem'~\citep{hochreiter1998vanishing}, making learning difficult and slow. However, the residual connections allow models to have hundreds of layers rather than tens of layers~\citep{resnet,vgg}. Despite the prevalence of the reference architecture, several modifications have been proposed that trade off, for instance, robustness and computational cost by varying the number of convolutional layers in the model. We tested the \textbf{Resnet-18}, \textbf{-34}, \textbf{-51}, \textbf{-101}, and \textbf{-152} reference architectures, that get their names from their respective number of layers. We used the the \texttt{pytorch} framework and the Stochastic Gradient Descent minimizer with a momentum parameter of 0.9 and learning rates in $\{10000, 100, 10, 1, 0.1, 0.01, 0.001, 0.000001\}$ for 20 epochs. The learning rate that maximized the benign accuracy for a given layer/defence configuration was used in further analyses (\textit{i.e.} the learning rate was tuned).

\subsection{Tested Defences}
\label{defences}
In order to simulate various conditions affecting the model's efficacy, we have also tested several defences that modify the model's inputs or predictions in an attempt to reduce its susceptibility to adversarial perturbations. Just like with the attacks, we used the Adversarial Robustness Toolbox~\citep{art2018} for their convenient implementations. These defences are listed below.

\begin{itemize}
    \item \textbf{Gauss-in} ($\ell_2$): the `Gaussian Augmentation' defence replaces some of the training samples with noisy samples drawn from a specified normal distribution, allowing us to simulate the effect of noise  on the resulting model~\citep{gauss_aug}. We tested levels in $\{.001, .01, .1, .3, .5, 1\}$.
    \item \textbf{Conf} : ($\ell_{\infty}$) the `High Confidence Thresholding' defence only returns a classification when the specified confidence threshold is reached, resulting in a failed query if a classification is less certain. This allows us to simulate the effects of rejecting `adversarial' or otherwise `confusing' queries~\citep{high_conf} that fall outside of given confidence range by ignoring ambiguous results without penalty. We tested confidence levels in $\{.1, .5, .9, .99, .999\}$.
    \item \textbf{Gauss-out} ($\ell_2$): the `Gaussian Noise` defence, rather than adding noise to the input data, it adds  noise to the model outputs~\citep{gauss_out}, allowing us to simulate small changes in model hyperparameters without going through costly training iterations. We tested levels in $\{.001, .01, .1, .3, .5, 1\}$.
    \item \textbf{FSQ} : the `Feature Squeezing` defence changes the bit-depth of the input data in order to minimize the noise induced by floating point operations. We include it here as a simulation of the effects of various GPU or CPU architectures, which may also vary in bit-depth~\citep{feature_squeezing} we tested bit-depths in $\{2, 4, 8, 16, 32, 64\}$
\end{itemize}

\subsection{Tested Attacks}
\label{attacks}
In order to simulate various attacks that vary in information and run-time requirements across a variety of distance metrics, we have evaluated several attacks using the Adversarial Robustness Toolbox~\citep{art2018}. Other researchers~\citep{carlini_towards_2017} have noted the importance of testing against multiple types of attacks. For our purposes, \textbf{attack strength} refers to the degree to which an input is modified by an attacker described in Sec.~\ref{perturbation_distance}. Below, we briefly describe the attacks that we tested against. One or more norms or pseudo-norms were used to optimise each attack, denoted in the parentheses next to the attack name.

\begin{itemize}
    \item \textbf{FGM} ($\ell_1, \ell_2, \ell_{\infty}$): the `Fast Gradient Method' quickly generates noisy sample with no feasibility conditions beyond a specified step size and number of iterations~\citep{fgm} by using the model gradient to find the point on the $\varepsilon$-sphere that maximizes the loss with $\varepsilon \in \{001,.01,.03,.1,.2,.3,.5,.8,.1\}$.
    \item \textbf{PGD}  ($\ell_1, \ell_2, \ell_{\infty}$):  the `Projected Gradient Method' extends the FGM attack to include a projection that imposes a feasibility limitation on the induced feature space, ensuring that generated samples do not fall outside of the feasible space~\citep{madry2017towards}. In short, this imposes feasibility conditions on the FGM attack with $\varepsilon \in \{001,.01,.03,.1,.2,.3,.5,.8,.1\}$.
    \item \textbf{Pixel} ($\ell_0$): the `Pixel' attack~\citep{pixelattack} is a multi-objective attack that seeks to minimize the number of perturbed pixels  while maximizing the false confidence using a classic multi-objective search function called NSGA2\citep{nsga2}. The control parameter for this was the number of perturbed pixels $\varepsilon \in \{4, 16, 64, 256\}$~.
    \item \textbf{Thresh} ($\ell_{\infty})$: the `Threshold' attack also uses the same multi-objective search algorithm as Pixel to optimize the attack, but tries to maximize false confidence while minimizing the $\ell_2$ perturbation distance.
    \item \textbf{Deep} ($\ell_2$): the Deepfool Attack~\citep{deepfool} finds the minimal separating hyperplane between two classes and then adds a specified amount of perturbation to ensure it crosses the boundary by using an approximation of the model gradient including the top $n$ outputs where $n \in [1,3,5,10]$, speeding up computation by ignoring unlikely classes~\citep{kotyan2022adversarial}.
    \item \textbf{HSJ} ($\ell_2$, \textit{queries}): the `HopSkipJump' attack, in contrast to the attacks above, does not need access to model gradients nor soft class labels, instead relying on an offline approximation using the model's decision boundaries. In this case, the strength is denoted by the number of queries necessary to find an adversarial counterexample~\citep{hopskipjump}.
    % \item \textbf{Knock-off:} The 'Knock-off-network' attack creates a proxy-model using a network architecture supplied by the attacker. By using a network with a smaller architecture (as we did), we use this to simulate an attacker who is trying to steal an approximation of the model.
    % \item \textbf{Poisoning:} the `Backdoor Poisoning' attack interferes with the training process by injecting adversarially perturbed samples such that they appear to be a particular class. We use this to simulate how these \textit{worst-case} perturbations to the training set influence the model's efficacy (samples or $\ell_0$ distance). 
    % \item \textbf{Membership:} the `Membership Inference` attack attempts to find the prototypical example for a target class. We include this to simulate an attacker who is attacking only a single class (samples or $\ell_0$ distance). 
\end{itemize}




\subsection{Identification of Defence- and Attack-Specific Covariates}
For each attack type, we identified the attack-specific distance metric (or pseudo-metric) outlined in Sec.~\ref{attacks}. To compare the effect of this measure against other attacks, the value was min-max scaled so that all values fall on the interval $[0,1]$. For defences, we did the same scaling. However, while a larger number always means more (marginal) noise in the case of attacks, a larger value for the FSQ defence indicates a larger bit-depth and more floating point error. For Gauss-in and Gauss-out, a larger number does indicate more noise, but a larger number for Conf indicates a larger rejection threshold for less-than-certain classifications, resulting in less low-confidence noise. 


\subsection{AFR Models}
We tested the Weibull, Log-Normal, and Log-Logistic AFR models using the \texttt{lifelines}~\citep{lifelines} package in Python, relying on the metrics outlined in Sec.~\ref{metrics} for comparison since they are widely used in the wider AFR literature \citep{aft_models}.
