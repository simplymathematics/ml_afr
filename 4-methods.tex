\section{Methodology}
\label{methods}
Below we outline the experiments performed and the hyper-parameter configurations across the various model architectures, model defences, and attacks. All experiments were conducted on Ubuntu 18.04 in a virtual machine running in a shared-host environment with one NVIDIA V100 GPU using Python 3.8.8. All configurations were tested in a grid search using \texttt{hydra} \citep{hydra} to manage the parameters, \texttt{dvc}~\citep{dvc} to ensure reproducibility, and \texttt{optuna}~\citep{optuna} to manage the scheduling. For each attack and model configuration, we collected the metrics outlined in Eqs.~\ref{eq:acc}--\ref{eq:cost} as well as inference time, training time, and attack generation time. We conducted a grid search of datasets, models, defences, and attacks across 10 shufflings of the data. To generate the figures, we selected the Pareto set~\cite{jahan2016multi} using the \texttt{paretoset}~\citep{paretoset} package, selecting the subset of experiments that either maximized the accuracy or minimzed the adversarial accuracy for each model, defence, and attack for each specified value. For visualization, we approximated $f_{ben.}$ and $f_{adv.}$ for each attack and defence combination using Eqs.~\ref{eq:ben_failure_rate} and approximated $\bar{C}$ in the adversarial and benign scenarios as per Eq.~\ref{eq:cost}

\subsection{Dataset}
\label{dataset}
Experiments were performed on both the CIFAR100, CIFAR10~\citep{cifar}, and MNIST~\citep{mnist} datasets. We measured the adversarial/benign accuracies, the attack generation time, and the prediction time. To calculate the adversarial failure rate and the cost we used Eqs.~\ref{eq:failure_rate}~\&~\ref{eq:cost}. For accuracy, see: Eq.~\ref{eq:acc}. We trained on 80\% of the samples for all datasets. Of the remaining 20\%, one-hundred class-balanced samples were selected from this test set to evaluate each attack. In addition, the data were shuffled to provide us with 10 samples for each configuration. Then, the data were centered and scaled by the using the parameters defined by the training set to avoid data leakage.

\subsection{Tested Models}
\label{models}
The Residual Neural Network (ResNet)~\citep{resnet} is a popular classification model\footnote{More than 180 thousand citations : \href{https://scholar.google.com/scholar?cites=9281510746729853742}{ResNet citations on Google Scholar.}} because of its ability to train neural networks with many layers efficiently through \textit{residual connections}.
\textit{Deep} networks, \textit{e.g.}, VGG~\citep{vgg}, ResNet~\citep{resnet}, rely on the depth of the network, which, despite leading to more accurate or robust results~\citep{rolnick2017power, carlini_towards_2017}, tends to lead to the `vanishing gradient problem'~\citep{hochreiter1998vanishing}, making learning difficult and slow. However, the residual connections allow models to have hundreds of layers rather than tens of layers~\citep{resnet,vgg}. Despite the prevalence of the reference architecture, several modifications have been proposed that trade off, for instance, robustness and computational cost by varying the number of convolutional layers in the model. We tested the \textbf{Resnet-18}, \textbf{-34}, \textbf{-51}, \textbf{-101}, and \textbf{-152} reference architectures, that get their names from their respective number of layers. We used the the \texttt{pytorch} framework and the Stochastic Gradient Descent minimizer with a momentum parameter of 0.9 and learning rates $\in \{10, 1, 0.1, 0.01, 0.001, 0.0001, .00001, 0.000001\}$ for epochs $\in \{ 10, 20, 30, 50, 100\}$. The learning rate that maximized the benign accuracy for a given layer/defence configuration was used in further analyses (\textit{i.e.}, the learning rate was tuned). In a separate experiment, we also evaluated the different Resnet models across epochs $\in \{10, 20, 30, 50, 100\}$. 

\subsection{Tested Defences}
\label{defences}

In order to simulate various conditions affecting the model's efficacy, we have also tested several defences that modify the model's inputs or predictions in an attempt to reduce its susceptibility to adversarial perturbations. Just like with the attacks, we used the Adversarial Robustness Toolbox~\citep{art2018} for their convenient implementations. These defences are listed below.

\textbf{Gauss-in} ($\ell_2$): The 'Gaussian Augmentation' defence adds Gaussian noise to some proportion of the training samples. In our case, we set this proportion to 50\%, allowing us to simulate the effect of noise on the resulting model~\citep{gauss_aug}. We tested noise levels in $\{.001, .01, .1, .3, .5, 1\}$.

\textbf{Conf} ($\ell_{\infty}$): The 'High Confidence Thresholding' defence only returns a classification when the specified confidence threshold is reached, resulting in a failed query if a classification is less certain. This allows us to simulate the effects of rejecting 'adversarial' or otherwise 'confusing' queries~\citep{high_conf} that fall outside the given confidence range by ignoring ambiguous results without penalty. We tested confidence levels in $\{.1, .5, .9, .99, .999\}$.

\textbf{Gauss-out} ($\ell_2$): The 'Gaussian Noise' defence, rather than adding noise to the input data, adds noise to the model outputs~\citep{gauss_out}, allowing us to simulate small changes in model weights without going through costly training iterations. We tested levels in $\{.001, .01, .1, .3, .5, 1\}$.

\textbf{FSQ}: The 'Feature Squeezing' defence changes the bit-depth of the input data to minimize the noise induced by floating-point operations. We include it here to simulate the effects of various GPU or CPU architectures, which may also vary in bit-depth~\citep{feature_squeezing}. We tested bit-depths in $\{2, 4, 8, 16, 32, 64\}$.


\subsection{Tested Attacks}
\label{attacks}

In order to simulate various attacks that vary in information and run-time requirements across a variety of distance metrics, we have evaluated several attacks using the Adversarial Robustness Toolbox~\citep{art2018}. Other researchers~\citep{carlini_towards_2017} have noted the importance of testing against multiple types of attacks. For our purposes, \textbf{attack strength} refers to the degree to which an input is modified by an attacker, as described in Sec.~\ref{perturbation_distance}. Below, we briefly describe the attacks that we tested against. One or more norms or pseudo-norms were used to optimize each attack, denoted in the parentheses next to the attack name.

\textbf{FGM} ($\ell_1, \ell_2, \ell_{\infty}$): The 'Fast Gradient Method' quickly generates a noisy sample with no feasibility conditions beyond a specified step size and number of iterations~\citep{fgm} by using the model gradient and taking a step of length $\varepsilon$ in the direction that maximizes the loss with $\varepsilon \in \{.001,.01,.03,.1,.2,.3,.5,.8,1\}$.

\textbf{PGD}  ($\ell_1, \ell_2, \ell_{\infty}$): The 'Projected Gradient Method' extends the FGM attack to include a projection on the $\varepsilon$-sphere, ensuring that generated samples do not fall outside of the feasible space~\citep{madry2017towards}. This algorithm is iterative, and we restricted it to ten such iterations. In short, this imposes feasibility conditions on the FGM attack with $\varepsilon \in \{.001,.01,.03,.1,.2,.3,.5,.8,1\}$.

\subsection{Identification of ResNet Model-, Defence- and Attack-Specific Covariates}
For each attack type, we identified the attack-specific distance metric (or pseudo-metric) outlined in Sec.~\ref{attacks}. To compare the effect of this measure against other attacks, the valued were min-max scaled so that all values fell on the interval $[0,1]$. For defences, we did the same scaling. However, while a larger number always means more (marginal) noise in the case of attacks, a larger value for the FSQ defence indicates a larger bit-depth and more floating point error. For Gauss-in and Gauss-out, a larger number does indicate more noise, but a larger number for Conf indicates a larger rejection threshold for less-than-certain classifications, resulting in less low-confidence noise. For the models we tracked the number of epochs and the number of layers as well as the training and inference times. 


\subsection{AFR Models}
We tested the Weibull, Log-Normal, and Log-Logistic AFR models using the \texttt{lifelines}~\citep{lifelines} package in Python, relying on the metrics outlined in Section~\ref{metrics} for comparison since they are widely used in the AFR literature \citep{aft_models}.
