\section{Background}


Much work has already gone into explaining the dangers of adversarial attacks on ML pipelines \citep{carlini_towards_2017,croce_reliable_2020,pixelattack,fgm,biggio_evasion_2013} though that work is limited to ad-hoc and posterior evaluations against limited sets of attack and defence parameters, leading to results that are, at best, overconfident~\citep{meyers,ma2020imbalanced}. Here, we are addressing \textbf{evasion}~\citep{carlini_towards_2017} attacks that attempt to induce a misclassification at run-time though our analysis extends to other types of attacks like database poisoning~\citep{biggio_poisoning_2013, saha2020hidden}, model inversion~\citep{choquette2021label,li2021membership}, or data stealing~\citep{orekondy2019knockoff}. This work formalizes methods for quantifying the adversarial failure rate of a model and comparing the efficacy of model changes in the context of fixed compute budget.

\subsection{Adversarial Attacks}

In the context of ML, an adversarial attack refers to deliberate and malicious attempts to manipulate the behavior of a model. These attacks are designed to change the model's behavior by introducing carefully crafted input data that can cause the model to make incorrect predictions or otherwise produce undesired outputs.
The goal of an attacker is often to exploit vulnerabilities in the model's decision-making process or to probe its weaknesses. These attacks can occur during various stages of the ML pipeline, including during training \citep{biggio_poisoning_2013}, inference\citep{biggio_evasion_2013}, or deployment \citep{santos2021universal}. For all sections below, we collect metrics on the benign (unperturbed) data and adversarial (perturbed) data. The abbreviations \textit{ben.} and \textit{adv.} are used throughout, respectively.


\subsubsection{Perturbation Distance}

\label{perturbation_distance}
The strength of an attack is generally thought of in terms of perturbation `distance'~\citep{croce_reliable_2020,chakraborty_adversarial_2018,pixelattack}. The perturbation distance, denoted by $\varepsilon$, quantifies the magnitude of the perturbation applied to a sample, $x$, when generating a new adversarial sample, $x'$. The definition is,
\[
    \varepsilon := \| x' - x \| \leq \varepsilon^*,
\]
where $\| \cdot \|$ denotes a norm or pseudo-norm (\textit{e.g.}, the Euclidean $\ell_2$ norm or the $\ell_0$ pseudo-norm). We denote by $\varepsilon^*$ the maximum allowed deviation or distortion from the original input while still being within the feasible sample space. For example, this might be one bit, one pixel, or one byte, depending on the test conditions. For more information on different optimization criteria, see Section~\ref{attacks}.


\subsubsection{Accuracy and Failure Rate}
The accuracy refers to the percentage or proportion of examples that cause the targeted ML model to misclassify or produce incorrect outputs and is a measure of the vulnerability or susceptibility of the model to noise-induced failures. A lower accuracy indicates a higher rate of misclassifications or incorrect predictions. The accuracy, $\lambda$, is defined as:
\begin{equation}
    \lambda := 1 -  \frac{\mathrm{False~Classifications}}{N},
    \label{eq:acc}
\end{equation}
where $N$ is the number of samples.

However, accuracy is known to vary with things like model complexity~\cite{vgg,resnet}, data resolution~\cite{feature_squeezing}, the number of samples~\cite{vapnik1994measuring}, the number of classes ~\cite{dohmatob_generalized_2019} or the amount of noise in the data~\cite{gauss_aug,gauss_out,dohmatob_generalized_2019}. Likewise, many of these factors can also influence an attack's run time. Instead, we can think in terms of \textbf{failure rate}:
\begin{equation}
    \mathrm{Failure~Rate} := \frac{\mathrm{False~Classifications}}{\Delta t} ,
    \label{eq:failure_rate}
\end{equation}
where $\Delta t$ is a time interval. This failure rate varies as a function of time, and we would like to model this behaviour with respect to various attributes and parameters of a model.
So, let $h$ be a function with model/attack parameters, $\theta$, such that $h_{\theta}$ describes the rate of failure at time $t$.
We can express the failure rate in terms of a \textbf{hazard function}, which can be expressed in terms of a point in time, $t$. It is defined as:
\begin{equation}
    h_{\theta}(t) := \lim_{ \Delta t \rightarrow 0} \frac{P(t \leq T < t + \Delta t \,|\, T \geq t)}{\Delta t},
    \label{eq:failure_rate_h}
\end{equation}
where $T$ is time until a false classification occurs, also referred to as \textbf{survival time}~\cite{kleinbaum1996survival}. To describe the computational efficacy of different model and attack configurations in general, we modelled the
probability of not observing a failure before a given time, $t$, giving the \textbf{cumulative survival function},
\begin{equation}
    S_{\theta}(t) := \exp \left\{- \int_0^{t} h_{\theta}(\tau) d\tau) \right\}.
\label{eq:cdf}
\end{equation}
The probability density of observing a failure at time, $t$, is~\cite{kleinbaum1996survival},
\begin{equation*}
    f(t; \theta) := h_{\theta}(t)S_{\theta}(t).
    \label{eq:pdf}
\end{equation*}
If we assume the probability of failure and time-to-failure is uniform across the samples for a particular set of covariates, we can estimate the probability of failure on the unperturbed test set,
\begin{equation}
    f_{ben.}(t; \theta) \approx \frac{1 - \lambda_{ben.}}{t_{predict}} \cdot t,
    \label{eq:ben_failure_rate}
\end{equation}
where $t_{predict}$ indicates the inference time per sample. As we can plainly see, at $t=t_{predict}$, this reduces to one minus the test-set \textbf{benign accuracy}. However, this is, at best, an optimistic lower bound to the the real-world failure rate~\cite{meyers,croce_reliable_2020}. To get a realistic picture of the worst-case scenario, we can look at $f$ in the presence of adversarial noise,
\begin{equation}
    f_{adv.}(t; \theta) \approx \frac{ 1 - \lambda_{adv.}}{t_{attack}} \cdot t,
    \label{eq:adv_failure_rate}
\end{equation}
where $t_{attack}$ refers to the attack generation time per sample. Hence, at $t=t_{attack}$ this reduces to one minus the \textbf{adversarial accuracy}.


\subsection{Cost}

\label{cost}
For our purposes, we assumed the cost, $C_{train}$, is proportional to the total training, $T_{train}$, the number of samples, $N$,  and the training time per sample, $t_{\mathrm{train}}$, such that the cost of training on hardware with a fixed time-cost, $C_h$, is
\[
    C_{train} := C_{h} \cdot T_{\mathrm{train}} = C_h \cdot t_{train} \cdot N,
\]
where $C_h$ is the cost per time unit of a particular piece of hardware. This assumption means that cost will scale linearly with per-sample training time or sample size. Analogously, we can discuss this in adversarial terms where $N$ is the number of samples. If we assume the same hardware cost as the model-builder then,
$$
    C_{attack} := C_{h} \cdot T_{\mathrm{attack}} = C_h \cdot t_{attack} \cdot N.
$$

If we assume that the training time and attack time are uniform across the samples and restricted to the same hardware, then we can say that
\begin{equation}
C_{train} \propto t_{train} \text{~~~and~~~} C_{attack} \propto t_{attack}.
\label{eq:naive_cost}
\end{equation}

Furthermore, a fast attack will be lower-bounded by $t_{predict}$, which is generally much smaller than the training time, $t_{train}$. Of course the long-term costs of deploying a model will be related to the inference cost, but a model is clearly broken if the cost of improving a model ($\propto t_{train}$) is much, much larger than the cost of finding a counterexample ($\propto t_{attack}$) within some imperceptible noise distance of untainted samples. However, this merely encapsulates the cost of a particular model architecture or signal pre-processing choice and doesn't consider the likelihood of noise induced failure. Our cost normalized metric is introduced below in Eq.~\ref{eq:cost} in Section~\ref{cost_normalization}. Before we can compare this cost to the failure rate, we must estimate the attack time per sample, or expected survival time, $\mathbb{E}[T]$. For that, we use an accelerated failure rate model.
