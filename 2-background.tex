\section{Background}
% For the sake of the reader, we provide a section for definitions and requisite background information in the following sections. Our work differs from 

Much work has already gone into explaining the dangers of adversarial attacks on ML pipelines \citep{carlini_towards_2017,croce_reliable_2020,pixelattack,fgm,biggio_evasion_2013} though that work is limited to ad-hoc and posterior evaluations against limited sets of attack and defence parameters, leading to results that are, at best, overconfident~\citep{meyers,ma2020imbalanced}. Here, we are addressing \textbf{evasion}~\citep{carlini_towards_2017} attacks that attempt to induce a misclassification at run-time though our analysis extends to other types of attacks like database poisoning~\citep{biggio_poisoning_2013, saha2020hidden}, model inversion~\citep{choquette2021label,li2021membership}, or data stealing~\citep{orekondy2019knockoff}. This work formalizes methods for quantifying the adversarial failure rate of a model and comparing the efficacy of model changes in the context of fixed compute budget.

% \subsection{Cloud Architectures}
% A \textbf{microservice} is the smallest component of a `cloud-native' software stack. In the context of machine learning, that might be a tool for either training, inference, pre-processing, sampling, or any other arbitrarily small part of the data pipeline. A microservice \textbf{mesh} is a network infrastructure or architectural pattern that provides a unified and scalable approach to managing communication between microservices in a distributed system. It serves as a dedicated layer that abstracts away the complexity of service-to-service communication, enabling efficient and reliable interactions among these services. A microservice mesh typically consists of a set of interconnected components or proxies deployed alongside the microservices within the system. These components facilitate various capabilities and functionalities essential for managing the communication between microservices. Kubernetes has become one of the largest open source projects on the code-sharing website Github \citep{k8s-size}, which provides a framework for managing, monitoring, and networking a self-scaling set of tools across arbitrary software and hardware architectures.

% \subsection{Machine Learning Pipelines}
% Machine Learning pipelines are often long-running and complex software tool-chains with many tunable hyperparameters. Managing, tracking, and controlling for various parameters is non trivial, but many management tools are available~\citep{dvc, hydra, k8s}. Data Version Control (dvc)\citep{dvc}, for example, combines git-like features with standard machine learning pipelines to guarantee reproducibility and reduce repeated runs. Hydra \citep{hydra} is a tool for managing complex configuration and arbitrary search algorithms across the hyper-parameters while parallelizing the search across multiple cores or multiple machines. Kubernetes \citep{k8s}, allows us to treat the hardware infrastructure as a tunable parameter by reducing hardware configuration to a .yaml file that can be deployed in the cloud or locally. In this work, we combined all three to guarantee reproducibility, manage thousands of independent trials, and to deploy all of this iIn general, a dataset is split into \textbf{training} and \textbf{test} sets. The training set is then used to determine the best configuration of a given model architecture on a given hardware architecture with the expectation that it will generalize both on the withheld test set and on new data generated by users via application programming interface (API) calls. To verify the training process, the test set validated against the \textit{inference} configuration of a model which may run on different hardware than the \textbf{training} configuration to reduce cost, latency, or power consumption.


\subsection{Adversarial Attacks}

In the context of ML, an adversarial attack refers to deliberate and malicious attempts to manipulate or fool ML models. These attacks are designed to deceive or mislead the model's behavior by introducing carefully crafted input data that can cause the model to make incorrect predictions or otherwise produce undesired outputs. The goal of an attacker is often to exploit vulnerabilities in the model's decision-making process or to probe its weaknesses. These attacks can occur during various stages of the ML pipeline, including during training \citep{biggio_poisoning_2013}, inference\citep{biggio_evasion_2013}, or deployment \citep{santos2021universal}.\footnote{For all sections below, we collect metrics on the benign (unperturbed) data and adversarial (perturbed) data. The abbreviations \textit{Ben.} and \textit{Adv.} are used throughout, respectively.}


\subsubsection{Perturbation Distance}

\label{perturbation_distance}
The strength of an attack is generally though of in terms of perturbation `distance'~\citep{croce_reliable_2020,chakraborty_adversarial_2018,pixelattack}. The perturbation distance, denoted by $\varepsilon$, quantifies the magnitude of the perturbation applied to a sample, $x$, when generating a new adversarial sample, $x'$. The perturbation distance is usually defined using some norm (or pseudo-norm). The definition is,
\[
    \varepsilon := \| x' - x \| \leq \varepsilon_{max},
\]
where $\| \cdot \|$ denotes a norm or pseudo-norm (\eg, the Euclidean $\ell_2$ norm or the $\ell_0$ pseudo-norm). We denote by $\varepsilon_{max}$ the maximum allowed deviation or distortion from the original input while still being within the feasible sample space.


\subsubsection{Accuracy and Failure Rate}
The accuracy, denoted by $h$, refers to the percentage or proportion of examples that cause the targeted ML model to misclassify or produce incorrect outputs and is a measure of the vulnerability or susceptibility of the model to noise-induced failures. A lower accuracy indicates a higher rate of misclassifications or incorrect predictions, signifying a weaker model in terms of \textbf{robustness} against noise-induced failures. The accuracy is defined as,
\begin{equation}
    \lambda := \frac{\textrm{False~Classifications}}{N},
    \label{eq:acc}
\end{equation}
where $N$ is the number of samples and can be measured on both the benign and adversarial sets. To penalize more time-intensive attacks, we modelled the distribution of these failures as a function of computational time such that the \textbf{hazard function}, $H$, is:
\begin{equation}
    H(t; \theta) =  \theta h(t; \theta),
\label{eq:cdf}
\end{equation}
where $\theta := \theta(x;\beta)$ describes the joint effect of the covariates , ${x}$, with model parameters, ${\beta}$. The function $h(t;\theta)$ refers to the probability of adversarial failure at time $t$, $t_{attack}$ refers to the attack generation time per sample, and $t_{predict}$ indicates the inference time per sample. The collected covariates include number of model layers, the perturbation distance, the inference time, the predict time, the learning rate, the failure rate, the training time, and the random state (see Sec.~\ref{attacks}). If we assume the the chance of failure is time-independent, we can estimate the \textbf{failure rate} or \textbf{instantaneous hazard function} as
\begin{equation}
    h_{adv.}(t; \theta) \approx \frac{\lambda_{adv.}}{t_{attack}}~~\mathrm{and}~~h_{ben.}(t; \theta) \approx \frac{\lambda_{ben.}}{t_{predict}}
    \label{eq:failure_rate}
\end{equation}
\subsection{Cost}\label{cost}

For our purposes, we assumed the cost, $C$, is proportional to the total training, $T_{train}$, the number of samples, $N$, and training time per sample, $t_{\mathrm{train}}$, such that the cost of training, $t_{\mathrm{train}}$, is
\[
    C = T_{\mathrm{train}} := t_{train} \cdot N.
\]




\subsubsection{Failure Rates and Cost Normalization}
We have defined all of the metrics in the section below.
\label{metrics}

In order to account for the marginal cost of a given model change, we also define $\bar{C}_{adv.}$, which is the expected number of adversarial failures that an attacker can generate during the time it takes to train a single sample\footnote{Consequently, if this number is larger than one, the average attacker has a time advantage the average model builder.} and $C_{ben.}$, which indicates the expected number of unperturbed samples that would be incorrectly classified in a single sample of training time, such that
\begin{equation}
    \bar{C}_{adv.} := h_{adv.}(t, \theta) \cdot t_{train}
     ~~\textrm{and}~~
     \bar{C}_{ben.} := h_{ben.}(t, \theta) \cdot t_{train}
\label{eq:cost}
\end{equation}


% to reflect the marginal performance gains of a method normalized by the training time for a given defence and model. A larger number indicates that more training time is required for the same failure rate while a smaller number indicates the opposite. Because both of these numbers are strictly non-negative, so is the metric.



% each defined below. AIC is defined as
% \[
%     \text{AIC} = 2p - 2 \log{L(\theta, x)},
% \]
% where $p$ is the number of estimated parameters and $L(\theta, x)$ is the value of the likelihood function for the model parameterized by $\theta$ over the set of samples $x$.
% A \textbf{likelihood function}, $L(\theta, x)$, for some data, $x$, and the parameters of a distribution, $\theta$, gives the probability of observing a sample $x$. The \textbf{concordance score} is a ratio of correctly ordered (concordant) pairs to comparable pairs such that a score of 0 is perfectly discordant, a score of 1 is perfectly concordant, and a score of .5 is the same as random chance~\citep{concordance_score}. The Bayesian information criterion, \textbf{BIC}, is defined as
% \[
%     \text{BIC} = p \log{n} - 2 \log{L(\theta, x)}
% \]
% where $p$ is the number of estimated model parameters, $n$ is the number of training samples, and $L$ is the likelihood score.


% \subsection{Pareto Optimality}

% Multi-objective optimization is critical to balancing the run-time performance with run-time requirements. One popular method for formulating cost functions in the context of operational cost is the Pareto front~\citep{}, $\mathcal{P}$. Let $\mathcal{P}$ be the the set of $n$ model configurations such that:
% \begin{align*}
%     \mathcal{P} &= \bigl\{ x \in \mathbb{R}^p \,|\, \nexists x' \in \mathbb{R}^p, f_i(x') > f_i(x), \forall i \in \{1, 2, \ldots, n\} \bigr\},
% \end{align*}
% where $f_i$ is one of many objectives that we seek to minimize (\textit{e.g.}, cost, response time, or loss).. While it's possible to find a set of solutions that finds the optimal value for one or more criteria, our goal is to model the asymptotic behavior of the model across different model, hyperparameter, and hardware configurations to minimize the cost of deploying the model while maximizing the number of processed queries in a given time period. In order to examine the \textbf{best-case} scenario for each of the tested configurations, we found the Pareto optimal points that maximized benign accuracy (best defence) and adversarial accuracy (best attack for a given defence), for each tested attack and defence. We also attempted to minimized the training time (for each defence) and the attack generation time (for each attack) as additional Pareto-optimality conditions.

