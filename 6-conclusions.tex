\section{Considerations}
The proposed survival and cost analysis  has some limitations that we have taken all efforts to minimise and/or mitigate.
In order to minimise timing jitter, we measured the process time for a batch of samples and then assumed that the time per sample was the measured processor time divided by the number of samples.
In order to examine a variety of different optimisation criteria for adversarial perturbations, we included several different attacks (see Section~\ref{attacks}) --- though the choice of attack is highly contextual.
We must also note that none of these attacks are run-time optimal and are, at best, an underestimate of the true adversarial failure rate~\cite{meyers}.
Likewise, testing all known defences would be computationally infeasible. So, in order to maximise the number of evaluations for each attack/defence combination, we focused only on the pre- and post-processing technique.
Techniques like adversarial retraining~\cite{croce_reliable_2020}, model transformation~\cite{papernot_distillation_2016}, and model regularisation~\cite{jakubovitz2018improving} were excluded due to their comparatively larger run-times. The equation Section~\ref{cost_normalization} reveals why techniques that significantly increase the training time might ultimately work against the model builder.
Even if one assumes there is a defence that has 99\% efficacy, rather than the, at best, 40\% efficacy indicated by the adversarial accuracy in Figure~\ref{fig:accuracies}, it would only reduce $\bar{C}_{\mathrm{adv}}$ by roughly two orders of magnitude.
While this would would be sufficient to meet the bare minimum requirement outlined in Equation~\ref{eq:cost}, in practice, attacks are often successful on even a small number of samples, whereas training often requires many orders of magnitude more, raising serious concerns about the efficacy of any of these models and defences in the presence of these simple adversaries.
Furthermore, state of-the art leaderboards
\footnote{\href{https://github.com/MadryLab/mnist\_challenge}{Madry's MNIST Challenge}}
\footnote{\href{https://ml.cs.tsinghua.edu.cn/adv\-bench/}{Croce's Robust Bench}}
show that a 99\% generalised adversarial accuracy is, at best, optimistic. Nevertheless, the goal of this work was not to produce a comprehensive evaluation of all known defences, but to develop a cost-aware framework for evaluating their efficacy against a set of adversaries.

\section{Conclusion}
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available.
By examining the role of the attacks, defences, and model depth in the context of adversarial failure rate, we were able to build a reliable and effective modelling framework that applies AFT models to deep neural networks.
Through statistical analysis (see Equations~\ref{eq:acc}--\ref{eq:cost} and Table~\ref{tab:afr_summary}), we show that this method is both effective and data-agnostic.  We use this model to demonstrate the efficacy of attack- and defence-tuning (see Figure~\ref{fig:afr_models}) to  explore the relationships between accuracy and adversarial robustness (Figure~\ref{fig:dummies}), showing that various model defences are ineffective on average and marginally better than the control at best.
By measuring the cost-normalised failure rate (see Section~\ref{cost} and Figure~\ref{fig:failures_per_train_time}), we show that robustness gains from deeper networks is driven by model latency more than inherent robustness (Figure~\ref{fig:afr_models}).
Our methods can easily extend to any other arbitrary collection of model pre-processing, training, tuning, attack and/or deployment parameters. In short, we provide a rigorous way to compare not only the relative robustness of a model, but of its cost effectiveness in response to an attacker.
Our measurements rigorously demonstrate  that the depth of a ResNet architecture does little to guarantee robustness while the community trends towards larger models~\cite{desislavov2021compute}, larger datasets~\cite{desislavov2021compute,bailly2022effects}, and increasingly marginal gains~\cite{sun2017revisiting}. This approach has two advantages over the traditional train-test split method.
First, it can be used to quantify the effects of covariates such as model depth or noise distance to compare the effect of model changes. Secondly, the train-test split methodology relies on an ever-larger number of samples to increase precision, whereas the survival time method is able to precisely and accurately compare models using only a small number of samples~\cite{schmoor2000sample,lachin1981introduction} relative to the many billions of samples required of the train/test split methodology and safety-critical standards~\cite{iso26262,IEC61508,IEC62034,meyers}.
In short, by generating worst-case examples (\textit{e.g.}, adversarial ones), we can test \textit{and compare} arbitrarily complex models \textit{before} they leave the lab, drive a car, predict the presence of cancer, or pilot a drone.
