\textit{}\section{Considerations}
This approach has some limitations that we have taken all efforts to minimize and/or mitigate. In order to minimize the risk of timing jitter, we measured the process time for a batch of samples and then assumed that each sample took the same amount of time such that the time per sample was the measured processor time divided by the number of samples. In order to examine a variety of different distance measures for adversarial perturbations, we included several different attacks (see Sec.~\ref{attacks}), each having different optimization criteria for the purpose of getting different views of the same mode--though the choice of attack is highly contextual. We must also note that none of these attacks are optimal and are, at best, and underestimate of the true adversarial failure rate~\citep{meyers}. Of course, further run-time optimizations are possible (for example, by using a different optimizer or search space algorithm), but this would be beyond the scope of this paper. The goal of this work was not to produce a state-of-the-art result but to develop a modelling framework for measuring and predicting the expected failure rate for a large number of proposed models and defences across a wide variety of attacks.

\section{Conclusion}
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. By examining the role of the \cm{attacks, defences, and model depth} in the context of adversarial failure rate, we were able to build a reliable \cm{and effective} modelling framework that applies accelerated failure time models to deep neural networks. Through statistical analysis (see Eq.~\ref{eq:acc}-~\ref{eq:cost} and Tables~\ref{tab:cifar}\&~\ref{tab:mnist}), we show that this method if both effective and data-agnostic.  We use this model to demonstrate the efficacy of attack- and defence-tuning (see Fig.~\ref{fig:afr_models}); to  explore the relationships between accuracy and adversarial robustness (Fig. \ref{fig:failure_rate}), showing that various model defences are ineffective on average and marginally better than the control at best.
By measuring the cost-normalized failure rate (see Sec.~\ref{cost} and Fig.~\ref{fig:failures_per_train_time}), we show that robustness gains from deeper networks is driven by model latency more than inherent robustness (Fig~\ref{fig:afr_models}). \cm{Our methods can easily extend to any other arbitrary collection of model pre-processing, training, tuning, attack and/or deployment parameters. In short, we provide a rigorous way to compare not only the relative robustness of a model, but of its cost effectiveness in response to an attacker. Our measurements rigorously demonstrate  that the depth of a ResNet architecture does little to guarantee robustness while the community trends towards larger models~\cite{}, larger datasets~\cite{}, and increasingly marginal gains~\cite{}.}
