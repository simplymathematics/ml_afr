\section{Considerations}
The proposed survival and cost analysis  has some limitations that we have taken all efforts to minimise and/or mitigate. 
In order to minimise timing jitter, we measured the process time for a batch of samples and then assumed that the time per sample was the measured processor time divided by the number of samples. 
In order to examine a variety of different optimisation criteria for adversarial perturbations, we included several different attacks (see Section~\ref{attacks})---though the choice of attack is highly contextual.
We must also note that none of these attacks are run-time optimal and are, at best, an underestimate of the true adversarial failure rate~\cite{meyers}. 
Likewise, testing all known defences would be computationally infeasible. 
As such, we focused only on the pre- and post-processing technique. 
% Techniques like adversarial retraining~\cite{croce_reliable_2020}, model transformation~\cite{papernot_distillation_2016}, and model regularisation~\cite{jakubovitz2018improving} were excluded due to their comparatively larger run-times. The equation Section~\ref{cost_normalization} reveals why techniques that significantly increase the training time might ultimately work against the model builder.
% Even if one assumes there is a defence that has 99\% efficacy, rather than the, at best, 40\% efficacy indicated by the adversarial accuracy in Figure~\ref{fig:accuracies}, it would only reduce $\bar{C}_{\mathrm{adv}}$ by roughly two orders of magnitude.
While every configuration of ResNet met the bare minimum requirement outlined in Equation~\ref{eq:cost}, real training processes require many thousands of samples and attacks are consistently successful with only one hundred samples. 
Together, these considerations raise serious concerns about the efficacy of any of these models and defences in the presence of these simple adversaries, meaning attacks will likely be many orders of magnitude cheaper than defences for tested configurations of this model.
Furthermore, state of-the art leaderboards\footnote{\href{https://github.com/MadryLab/mnist\_challenge}{Madry's MNIST Challenge}}\footnote{\href{https://ml.cs.tsinghua.edu.cn/adv\-bench/}{Croce's Robust Bench}}
show that a 99\% generalised adversarial accuracy is, at best, optimistic. 
% Nevertheless, the goal of this work was not to produce a comprehensive evaluation of all known defences, but to develop a cost-aware framework for evaluating their efficacy against a set of adversaries.

\section{Conclusion}
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available.
By examining the role of the attacks, defences, and model depth in the context of adversarial failure rate, this paper presents a reliable and effective modelling framework that applies AFT models to deep neural networks.
The metrics outlined Table~\ref{tab:aft_summary} and explained in Section~\ref{metrics} show that this method is both effective and data-agnostic.  
We use this model to demonstrate the efficacy of various attack- and defence-tuning techniques, to  explore the relationships between accuracy and adversarial robustness (Figure~\ref{fig:dummies}), and show that various model defences are ineffective on average and marginally better than the control at best.
By measuring the cost-normalised failure rate or TRASH score (see Section~\ref{cost_normalization} and Figure~\ref{fig:trash}), it is clear that all tested configurations of ResNet fail to meet the TRASH criterion.
The methods can easily extend to any other arbitrary collection of model pre-processing, training, tuning, attack and/or deployment parameters. 
In short, AFTs provide a rigorous way to compare not only the relative robustness of a model, but of its cost effectiveness in response to an attacker.
The measurements rigorously demonstrate  that the depth of a ResNet architecture does little to guarantee robustness while the community trends towards larger models~\cite{desislavov2021compute}. 

While the train-test split methodology relies on an ever-larger number of samples to increase precision, the survival time method is able to precisely and accurately compare models using only a small number of samples~\cite{schmoor2000sample,lachin1981introduction} relative to the many billions of samples required of the train/test split methodology and safety-critical standards~\cite{iso26262,IEC61508,IEC62034,meyers}.
In short, by generating worst-case examples (\textit{e.g.}, adversarial ones), one can test and compare arbitrarily complex models \textit{before} they leave the lab, drive a car, predict the presence of cancer, or pilot a drone.
