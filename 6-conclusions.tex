\section{Considerations}
The proposed survival and cost analysis  has some limitations that we have taken all efforts to minimize and/or mitigate. In order to minimize timing jitter, we measured the process time for a batch of samples and then assumed that the time per sample was the measured processor time divided by the number of samples. In order to examine a variety of different optimization criteria for adversarial perturbations, we included several different attacks (see Sec.~\ref{attacks})---though the choice of attack is highly contextual. We must also note that none of these attacks are run-time optimal and are, at best, an underestimate of the true adversarial failure rate~\citep{meyers}. Of course, further run-time optimizations are possible, but this would be beyond the scope of this paper. \cm{Likewise, testing all known defences would be computationally impossible. So, in order to maximize the number of evaluations for each attack/defence combination, we focused only on the pre- and post-processing technique. Techniques like adversarial retraining ~\cite{croce_reliable_2020}, model transformation ~\cite{papernot_distillation_2016}, and model regularization ~\cite{jakubovitz2018improving} were excluded due to their comparatively larger run-times. The equation Section~\ref{cost_normalization} reveals why techniques that significantly increase the training time might ultimately work against the model builder. However, the goal of this work was not produce a comprehensive evaluation of all known attacks and defences, but to develop a cost-aware framework for evaluating their efficacy against an adversary.}
% One such optimization is running evaluations on GPUs with a smaller physical bit-depth than a V100, since FSQ doesn't seem to hinder the model significantly (see Fig.~\ref{fig:dummies}). This could yield a tremendous power savings over standard server-grade GPUs~\citep{chou2023applicability}.
% However, the goal of this work was not to produce a perfect measure of failure rate but to develop a technique for measuring and predicting the expected adversarial failure rate for a large number of proposed models and defences across a wide variety of attacks on a targeted piece of hardware.

\section{Conclusion}
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. By examining the role of the attacks, defences, and model depth in the context of adversarial failure rate, we were able to build a reliable and effective modelling framework that applies accelerated failure rate models to deep neural networks. Through statistical analysis (see Eq.~\ref{eq:acc}--\ref{eq:cost} and Table~\ref{tab:afr_models}), we show that this method is both effective and data-agnostic.  We use this model to demonstrate the efficacy of attack- and defence-tuning (see Fig.~\ref{fig:afr_models}) to  explore the relationships between accuracy and adversarial robustness (Fig.~\ref{fig:dummies}), showing that various model defences are ineffective on average and marginally better than the control at best.
By measuring the cost-normalized failure rate (see Sec.~\ref{cost} and Fig.~\ref{fig:failures_per_train_time}), we show that robustness gains from deeper networks is driven by model latency more than inherent robustness (Fig.~\ref{fig:afr_models}). Our methods can easily extend to any other arbitrary collection of model pre-processing, training, tuning, attack and/or deployment parameters. In short, we provide a rigorous way to compare not only the relative robustness of a model, but of its cost effectiveness in response to an attacker. Our measurements rigorously demonstrate  that the depth of a Resnet architecture does little to guarantee robustness while the community trends towards larger models~\cite{desislavov2021compute}, larger datasets~\cite{desislavov2021compute,bailly2022effects}, and increasingly marginal gains~\cite{sun2017revisiting}.
